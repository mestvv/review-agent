"""Streamlit –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Review Agent."""

import json
import logging
import re
from datetime import datetime
from typing import Optional

import streamlit as st

from src.agent.agent import (
    create_llm,
    stream_llm_answer_qa,
    stream_llm_answer_review,
    _save_response_to_json,
    _save_to_markdown,
)
from src.agent.agent_with_tools import (
    create_rag_agent,
    _save_chat_session_to_json,
    _save_chat_session_to_markdown,
)
from src.agent.prompts import QA_PROMPT, REVIEW_PROMPT
from langchain_core.messages import HumanMessage, AIMessage
from src.config import (
    list_existing_dbs,
    list_available_dbs,
    get_articles_subdir,
    CHUNKS_LOG_DIR,
    EXPAND_WINDOW,
    EXPAND_TOP_N,
    LLM_TEMPERATURE,
)
from src.rag import (
    index_all_pdfs,
    clear_database,
    retrieve_chunks,
    retrieve_with_reranking,
    get_neighbor_chunks,
    format_context_with_citations,
    calculate_confidence,
    RetrievedChunk,
    ConfidenceScore,
    ConfidenceLevel,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Review Agent",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded",
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏ –¥–ª—è —á–∞—Ç–∞ —Å –∞–≥–µ–Ω—Ç–æ–º
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []  # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–∞ (LangChain messages)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
if "chat_session_start" not in st.session_state:
    st.session_state.chat_session_start = None
if "chat_agent" not in st.session_state:
    st.session_state.chat_agent = None


def _save_chunks_to_json(
    chunks: list[RetrievedChunk],
    query: str,
    expanded_chunks: Optional[list[RetrievedChunk]] = None,
) -> str:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSON –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É."""
    CHUNKS_LOG_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    filepath = CHUNKS_LOG_DIR / f"chunks_{timestamp}_{safe_query}.json"

    expanded_ids = set()
    if expanded_chunks:
        expanded_ids = {f"{c.file_hash}_{c.chunk_id}" for c in expanded_chunks}

    chunks_data = []
    for chunk in chunks:
        chunk_key = f"{chunk.file_hash}_{chunk.chunk_id}"
        chunks_data.append(
            {
                "chunk_id": chunk.chunk_id,
                "file_name": chunk.file_name,
                "file_hash": chunk.file_hash,
                "page": chunk.page,
                "section": chunk.section,
                "distance": chunk.distance,
                "text": chunk.text,
                "is_expanded": chunk_key in expanded_ids,
            }
        )

    data = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "total_chunks": len(chunks),
        "expanded_chunks_count": len(expanded_chunks) if expanded_chunks else 0,
        "sources": sorted(set(c.file_name for c in chunks)),
        "chunks": chunks_data,
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return str(filepath)


def _format_confidence_dict(confidence: ConfidenceScore) -> dict:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç ConfidenceScore –≤ —Å–ª–æ–≤–∞—Ä—å."""
    level_text = {
        ConfidenceLevel.HIGH: "–í—ã—Å–æ–∫–∞—è",
        ConfidenceLevel.MEDIUM: "–°—Ä–µ–¥–Ω—è—è",
        ConfidenceLevel.LOW: "–ù–∏–∑–∫–∞—è",
        ConfidenceLevel.VERY_LOW: "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
    }

    return {
        "level": level_text.get(confidence.level, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"),
        "level_raw": (
            confidence.level.value
            if hasattr(confidence.level, "value")
            else str(confidence.level)
        ),
        "score": confidence.score,
        "num_sources": confidence.num_sources,
        "num_chunks": confidence.num_chunks,
        "avg_distance": confidence.avg_distance,
        "coverage_by_section": confidence.coverage_by_section,
        "warnings": confidence.warnings,
    }


def _create_response_object(content: str):
    """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç-–æ–±–µ—Ä—Ç–∫—É –¥–ª—è –æ—Ç–≤–µ—Ç–∞ LLM –∏–∑ —Å—Ç—Ä–æ–∫–∏.

    Args:
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞

    Returns:
        –û–±—ä–µ–∫—Ç —Å –º–µ—Ç–æ–¥–æ–º dict() –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å _save_response_to_json
    """

    class ResponseWrapper:
        def __init__(self, content: str):
            self.content = content

        def dict(self):
            return {"content": self.content}

    return ResponseWrapper(content)


def _format_confidence_for_prompt(confidence: ConfidenceScore, query_type: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç confidence –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
    level_text = {
        ConfidenceLevel.HIGH: "–í–´–°–û–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –º–æ–∂–Ω–æ –¥–∞–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
        ConfidenceLevel.MEDIUM: "–°–†–ï–î–ù–Ø–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏",
        ConfidenceLevel.LOW: "–ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
        ConfidenceLevel.VERY_LOW: "–û–ß–ï–ù–¨ –ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω, —è–≤–Ω–æ —Å–æ–æ–±—â–∏ –æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
    }

    info = f"""–£—Ä–æ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {level_text.get(confidence.level, '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}
–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources}
–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {confidence.avg_distance:.3f}"""

    if confidence.warnings:
        info += f"\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {'; '.join(confidence.warnings)}"
    return info


def answer_question_web(
    question: str,
    db_name: str,
    n_results: int = 5,
    expand_context: bool = True,
    temperature: Optional[float] = None,
    streaming: bool = False,
):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.

    Args:
        question: –í–æ–ø—Ä–æ—Å
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        expand_context: –†–∞—Å—à–∏—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        streaming: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä)

    Returns:
        –ï—Å–ª–∏ streaming=False: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        –ï—Å–ª–∏ streaming=True: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """
    initial_chunks, query_type, confidence = retrieve_with_reranking(
        question, db_name, n_results, fetch_multiplier=2
    )

    if not initial_chunks:
        if streaming:
            return None, {"success": False, "error": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"}
        return {
            "success": False,
            "error": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω",
        }

    # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥—è–º–∏
    expanded_chunks = []
    if expand_context:
        seen_ids = {f"{c.file_hash}_{c.chunk_id}" for c in initial_chunks}
        top_n_for_expansion = min(EXPAND_TOP_N, len(initial_chunks))
        for chunk in initial_chunks[:top_n_for_expansion]:
            neighbors = get_neighbor_chunks(
                chunk, db_name, window=EXPAND_WINDOW, query=question
            )
            for n in neighbors:
                key = f"{n.file_hash}_{n.chunk_id}"
                if key not in seen_ids:
                    expanded_chunks.append(n)
                    seen_ids.add(key)
        chunks = (
            initial_chunks[:top_n_for_expansion]
            + expanded_chunks
            + initial_chunks[top_n_for_expansion:]
        )
    else:
        chunks = initial_chunks

    _save_chunks_to_json(chunks, question, expanded_chunks if expand_context else None)

    context = format_context_with_citations(chunks[:10])
    confidence_info = _format_confidence_for_prompt(confidence, query_type)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = {}
    for chunk in chunks:
        if chunk.file_name not in sources:
            sources[chunk.file_name] = {
                "pages": set(),
                "sections": set(),
            }
        sources[chunk.file_name]["pages"].add(chunk.page)
        sources[chunk.file_name]["sections"].add(chunk.section)

    sources_list = []
    for source_name, info in sorted(sources.items()):
        sources_list.append(
            {
                "file_name": source_name,
                "pages": sorted(info["pages"]),
                "sections": sorted(info["sections"]),
            }
        )

    metadata = {
        "success": True,
        "confidence": _format_confidence_dict(confidence),
        "query_type": query_type,
        "sources": sources_list,
        "chunks_count": len(chunks),
    }

    if streaming:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (—á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        full_answer_container = [""]

        def answer_generator():
            for token in stream_llm_answer_qa(
                question, context, confidence_info, temperature
            ):
                full_answer_container[0] += token
                yield token
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            full_answer = full_answer_container[0]
            metadata["answer"] = full_answer

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            query_data = {
                "question": question,
                "context": context,
                "confidence": confidence.to_dict(),
            }
            response_obj = _create_response_object(full_answer)
            _save_response_to_json(query_data, response_obj)
            _save_to_markdown(
                query=question,
                response_content=full_answer,
                chunks=chunks,
                query_type="ask",
                confidence=confidence,
            )

        return answer_generator(), metadata
    else:
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑ streaming
        llm = create_llm(temperature=temperature)
        response = (QA_PROMPT | llm).invoke(
            {
                "question": question,
                "context": context,
                "confidence_info": confidence_info,
            }
        )
        metadata["answer"] = response.content

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏
        query_data = {
            "question": question,
            "context": context,
            "confidence": confidence.to_dict(),
        }
        _save_response_to_json(query_data, response)
        _save_to_markdown(
            query=question,
            response_content=response.content,
            chunks=chunks,
            query_type="ask",
            confidence=confidence,
        )

        return metadata


def review_topic_web(
    topic: str,
    db_name: str,
    n_results: int = 15,
    temperature: Optional[float] = None,
    streaming: bool = False,
):
    """–°–æ–∑–¥–∞–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.

    Args:
        topic: –¢–µ–º–∞ –æ–±–∑–æ—Ä–∞
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        streaming: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä)

    Returns:
        –ï—Å–ª–∏ streaming=False: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        –ï—Å–ª–∏ streaming=True: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """
    from src.rag.retriever import detect_query_type

    query_type = detect_query_type(topic)
    all_chunks, query_type, confidence = retrieve_with_reranking(
        topic, db_name, n_results, fetch_multiplier=2
    )

    if not all_chunks:
        if streaming:
            return None, {"success": False, "error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–∑–æ—Ä–∞"}
        return {
            "success": False,
            "error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–∑–æ—Ä–∞",
        }

    confidence = calculate_confidence(all_chunks, query_type)
    _save_chunks_to_json(all_chunks, topic)

    context = format_context_with_citations(all_chunks)

    sources_detail = []
    seen = set()
    for chunk in all_chunks:
        if chunk.file_name not in seen:
            sources_detail.append(f"‚Ä¢ {chunk.file_name}")
            seen.add(chunk.file_name)

    confidence_info = _format_confidence_for_prompt(confidence, query_type)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = {}
    for chunk in all_chunks:
        if chunk.file_name not in sources:
            sources[chunk.file_name] = {
                "pages": set(),
                "sections": set(),
            }
        sources[chunk.file_name]["pages"].add(chunk.page)
        sources[chunk.file_name]["sections"].add(chunk.section)

    sources_list = []
    for source_name, info in sorted(sources.items()):
        sources_list.append(
            {
                "file_name": source_name,
                "pages": sorted(info["pages"]),
                "sections": sorted(info["sections"]),
            }
        )

    metadata = {
        "success": True,
        "confidence": _format_confidence_dict(confidence),
        "query_type": query_type,
        "sources": sources_list,
        "chunks_count": len(all_chunks),
    }

    if streaming:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞ (—á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        full_review_container = [""]

        def review_generator():
            for token in stream_llm_answer_review(
                topic,
                context[:8000],
                "\n".join(sources_detail),
                confidence_info,
                temperature,
            ):
                full_review_container[0] += token
                yield token
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            full_review = full_review_container[0]
            metadata["review"] = full_review

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            query_data = {
                "topic": topic,
                "context": context[:8000],
                "confidence": confidence.to_dict(),
            }
            response_obj = _create_response_object(full_review)
            _save_response_to_json(query_data, response_obj)
            _save_to_markdown(
                query=topic,
                response_content=full_review,
                chunks=all_chunks,
                query_type="review",
                confidence=confidence,
            )

        return review_generator(), metadata
    else:
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑ streaming
        llm = create_llm(temperature=temperature)
        response = (REVIEW_PROMPT | llm).invoke(
            {
                "topic": topic,
                "context": context[:8000],
                "sources": "\n".join(sources_detail),
                "confidence_info": confidence_info,
            }
        )
        metadata["review"] = response.content

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏
        query_data = {
            "topic": topic,
            "context": context[:8000],
            "confidence": confidence.to_dict(),
        }
        _save_response_to_json(query_data, response)
        _save_to_markdown(
            query=topic,
            response_content=response.content,
            chunks=all_chunks,
            query_type="review",
            confidence=confidence,
        )

        return metadata


def get_stats_dict(db_name: Optional[str] = None) -> dict:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è."""
    from src.rag.indexer import _get_collection

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ –º–æ–¥—É–ª—è
    import src.config as config

    if db_name:
        collection = _get_collection(db_name)
        total = collection.count()
        if total == 0:
            return {"db_name": db_name, "total": 0, "files": {}, "sections": {}}

        results = collection.get(include=["metadatas"])
        metadatas = results["metadatas"]

        files = {}
        sections = {}
        for metadata in metadatas:
            fname = metadata.get("file_name", "unknown")
            section = metadata.get("section", "unknown")
            files[fname] = files.get(fname, 0) + 1
            sections[section] = sections.get(section, 0) + 1

        return {
            "db_name": db_name,
            "total": total,
            "files": files,
            "sections": sections,
        }
    else:
        existing_dbs_list = config.list_existing_dbs()
        stats_dict = {}
        total_chunks = 0
        for db_item in existing_dbs_list:
            collection = _get_collection(db_item)
            count = collection.count()
            total_chunks += count
            stats_dict[db_item] = {"total": count}

        return {
            "all_dbs": stats_dict,
            "total_chunks": total_chunks,
        }


# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ë–î
with st.sidebar:
    st.title("üìö Review Agent")
    st.markdown("---")

    st.subheader("–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")

    existing_dbs = list_existing_dbs()
    available_dbs = list_available_dbs()

    if existing_dbs:
        selected_db = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö",
            existing_dbs,
            key="selected_db",
        )
    else:
        st.warning("–ù–µ—Ç –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö")
        selected_db = None

    st.markdown("---")

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ë–î
    st.subheader("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")

    with st.expander("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"):
        if st.button("–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É", use_container_width=True):
            if selected_db:
                stats = get_stats_dict(selected_db)
                st.json(stats)
            else:
                stats = get_stats_dict()
                st.json(stats)

    with st.expander("üîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        db_to_index = st.selectbox(
            "–ë–∞–∑–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏",
            [None] + available_dbs,
            key="db_to_index",
        )
        if st.button("–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å", use_container_width=True):
            if db_to_index:
                with st.spinner(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã '{db_to_index}'..."):
                    try:
                        index_all_pdfs(db_name=db_to_index)
                        st.success(f"–ë–∞–∑–∞ '{db_to_index}' –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞: {e}")
            else:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

    with st.expander("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ"):
        db_to_delete = st.selectbox(
            "–ë–∞–∑–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è",
            [None] + existing_dbs,
            key="db_to_delete",
        )
        if st.button("–£–¥–∞–ª–∏—Ç—å", use_container_width=True, type="primary"):
            if db_to_delete:
                try:
                    clear_database(db_name=db_to_delete)
                    st.success(f"–ë–∞–∑–∞ '{db_to_delete}' —É–¥–∞–ª–µ–Ω–∞!")
                    st.rerun()
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞: {e}")
            else:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")

# –ì–ª–∞–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
st.title("üìö Review Agent")
st.markdown("RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ–±–∑–æ—Ä–æ–≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")

if not selected_db:
    st.info("üëà –í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é")
    st.markdown("---")
    st.subheader("–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    if available_dbs:
        for db in available_dbs:
            articles_subdir = get_articles_subdir(db)
            pdf_count = len(list(articles_subdir.glob("*.pdf")))
            md_count = len(list(articles_subdir.glob("*.md")))
            docx_count = len(list(articles_subdir.glob("*.docx")))
            st.write(f"**{db}**: {pdf_count} PDF, {md_count} MD, {docx_count} DOCX")
    else:
        st.warning(
            "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π. –°–æ–∑–¥–∞–π—Ç–µ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø–∞–ø–∫–µ `articles/`"
        )
else:
    # –í–∫–ª–∞–¥–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
    tab1, tab2, tab3, tab4 = st.tabs(
        ["üí¨ –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å", "üìù –û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "üîç –ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤", "ü§ñ –ß–∞—Ç —Å –∞–≥–µ–Ω—Ç–æ–º"]
    )

    with tab1:
        st.subheader("–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")

        question = st.text_area(
            "–í–∞—à –≤–æ–ø—Ä–æ—Å",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –° –∫–∞–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ?",
            height=100,
        )

        col1, col2 = st.columns(2)
        with col1:
            n_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤", 1, 20, 5)
        with col2:
            expand_context = st.checkbox("–†–∞—Å—à–∏—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç", value=True)

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=2.0,
            value=LLM_TEMPERATURE,
            step=0.1,
            help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, –≤—ã—Å–æ–∫–∏–µ - –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º–∏",
            key="ask_temperature",
        )

        if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–æ–ø—Ä–æ—Å", type="primary", use_container_width=True):
            if question:
                # –ü–æ–ª—É—á–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
                answer_gen, metadata = answer_question_web(
                    question,
                    selected_db,
                    n_results=n_results,
                    expand_context=expand_context,
                    temperature=temperature,
                    streaming=True,
                )

                if metadata and metadata.get("success"):
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                    confidence = metadata["confidence"]
                    level_colors = {
                        "–í—ã—Å–æ–∫–∞—è": "üü¢",
                        "–°—Ä–µ–¥–Ω—è—è": "üü°",
                        "–ù–∏–∑–∫–∞—è": "üü†",
                        "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è": "üî¥",
                    }
                    icon = level_colors.get(confidence["level"], "‚ö™")

                    st.markdown("---")
                    st.markdown(f"### {icon} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence['level']}")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤", confidence["num_sources"])
                    with col2:
                        st.metric("–ß–∞–Ω–∫–æ–≤", confidence["num_chunks"])
                    with col3:
                        st.metric(
                            "–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è", f"{confidence['avg_distance']:.3f}"
                        )

                    # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
                    st.markdown("---")
                    st.markdown("### –û—Ç–≤–µ—Ç")

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º st.write_stream –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
                    # st.write_stream –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç UI –ø–æ –º–µ—Ä–µ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
                    full_answer = st.write_stream(answer_gen)

                    # –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ metadata —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä

                    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                    if metadata.get("sources"):
                        st.markdown("---")
                        st.markdown("### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
                        for source in metadata["sources"]:
                            pages_str = ", ".join(map(str, source["pages"]))
                            sections_str = (
                                ", ".join(source["sections"])
                                if source["sections"]
                                else "‚Äî"
                            )
                            with st.expander(f"üìÑ {source['file_name']}"):
                                st.write(f"**–°—Ç—Ä–∞–Ω–∏—Ü—ã:** {pages_str}")
                                st.write(f"**–°–µ–∫—Ü–∏–∏:** {sections_str}")
                else:
                    error_msg = (
                        metadata.get("error", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞")
                        if metadata
                        else "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞"
                    )
                    st.error(error_msg)
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")

    with tab2:
        st.subheader("–û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")

        topic = st.text_area(
            "–¢–µ–º–∞ –æ–±–∑–æ—Ä–∞",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤–µ—á–Ω–æ–π –º–µ—Ä–∑–ª–æ—Ç—ã –≤ –†–æ—Å—Å–∏–∏",
            height=100,
        )

        col1, col2 = st.columns(2)
        with col1:
            n_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤", 5, 30, 15)
        with col2:
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=LLM_TEMPERATURE,
                step=0.1,
                help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, –≤—ã—Å–æ–∫–∏–µ - –±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º–∏",
                key="review_temperature",
            )

        if st.button("–°–æ–∑–¥–∞—Ç—å –æ–±–∑–æ—Ä", type="primary", use_container_width=True):
            if topic:
                # –ü–æ–ª—É—á–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
                review_gen, metadata = review_topic_web(
                    topic,
                    selected_db,
                    n_results=n_results,
                    temperature=temperature,
                    streaming=True,
                )

                if metadata and metadata.get("success"):
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                    confidence = metadata["confidence"]
                    level_colors = {
                        "–í—ã—Å–æ–∫–∞—è": "üü¢",
                        "–°—Ä–µ–¥–Ω—è—è": "üü°",
                        "–ù–∏–∑–∫–∞—è": "üü†",
                        "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è": "üî¥",
                    }
                    icon = level_colors.get(confidence["level"], "‚ö™")

                    st.markdown("---")
                    st.markdown(f"### {icon} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence['level']}")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤", confidence["num_sources"])
                    with col2:
                        st.metric("–ß–∞–Ω–∫–æ–≤", confidence["num_chunks"])
                    with col3:
                        st.metric(
                            "–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è", f"{confidence['avg_distance']:.3f}"
                        )

                    # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ –æ–±–∑–æ—Ä–∞
                    st.markdown("---")
                    st.markdown("### –û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º st.write_stream –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
                    full_review = st.write_stream(review_gen)

                    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                    if metadata.get("sources"):
                        st.markdown("---")
                        st.markdown("### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
                        for source in metadata["sources"]:
                            pages_str = ", ".join(map(str, source["pages"]))
                            sections_str = (
                                ", ".join(source["sections"])
                                if source["sections"]
                                else "‚Äî"
                            )
                            with st.expander(f"üìÑ {source['file_name']}"):
                                st.write(f"**–°—Ç—Ä–∞–Ω–∏—Ü—ã:** {pages_str}")
                                st.write(f"**–°–µ–∫—Ü–∏–∏:** {sections_str}")
                else:
                    error_msg = (
                        metadata.get("error", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞")
                        if metadata
                        else "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞"
                    )
                    st.error(error_msg)
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –æ–±–∑–æ—Ä–∞")

    with tab3:
        st.subheader("–ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤")

        query = st.text_input(
            "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å",
            placeholder="–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞",
        )

        col1, col2 = st.columns(2)
        with col1:
            n_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", 1, 50, 10)
        with col2:
            section = st.selectbox(
                "–§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏",
                [
                    None,
                    "abstract",
                    "introduction",
                    "methods",
                    "results",
                    "discussion",
                    "conclusion",
                ],
            )

        if st.button("–ü–æ–∏—Å–∫", type="primary", use_container_width=True):
            if query:
                with st.spinner("–ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤..."):
                    chunks = retrieve_chunks(query, selected_db, n_results, section)

                    if chunks:
                        st.markdown(f"### –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
                        st.markdown("---")

                        for i, chunk in enumerate(chunks, 1):
                            with st.expander(
                                f"–ß–∞–Ω–∫ {i}: {chunk.file_name} (—Å—Ç—Ä. {chunk.page}, —Å–µ–∫—Ü–∏—è: {chunk.section}, dist: {chunk.distance:.3f})"
                            ):
                                st.text(chunk.text)
                    else:
                        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")

    with tab4:
        st.subheader("ü§ñ –ß–∞—Ç —Å RAG-–∞–≥–µ–Ω—Ç–æ–º")
        st.markdown(
            """
            –ê–≥–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.
            –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏.
            """
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≥–µ–Ω—Ç–∞
        with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏", expanded=False):
            agent_temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=LLM_TEMPERATURE,
                step=0.1,
                help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
                key="agent_temperature",
            )

        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é", use_container_width=True):
                st.session_state.chat_messages = []
                st.session_state.chat_history = []
                st.session_state.chat_agent = None
                st.session_state.chat_session_start = None
                st.rerun()
        with col2:
            if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–µ—Å—Å–∏—é", use_container_width=True):
                if st.session_state.chat_history:
                    _save_chat_session_to_json(
                        st.session_state.chat_history,
                        selected_db,
                        st.session_state.chat_session_start or datetime.now(),
                    )
                    _save_chat_session_to_markdown(
                        st.session_state.chat_history,
                        selected_db,
                        st.session_state.chat_session_start or datetime.now(),
                    )
                    st.success(f"–°–µ—Å—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({len(st.session_state.chat_history)} –æ–±–º–µ–Ω–æ–≤)")
                else:
                    st.warning("–ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞")
        with col3:
            st.metric("–°–æ–æ–±—â–µ–Ω–∏–π", len(st.session_state.chat_messages))

        st.markdown("---")

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        chat_container = st.container()
        with chat_container:
            for exchange in st.session_state.chat_history:
                # –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                with st.chat_message("user"):
                    st.markdown(exchange["question"])
                # –û—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞
                with st.chat_message("assistant"):
                    st.markdown(exchange["response"])
                    st.caption(f"üîß –í—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {exchange['tool_calls_count']}")

        # –ü–æ–ª–µ –≤–≤–æ–¥–∞ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –∞–≥–µ–Ω—Ç—É..."):
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if st.session_state.chat_agent is None:
                st.session_state.chat_agent = create_rag_agent(agent_temperature)
                st.session_state.chat_session_start = datetime.now()

            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            with st.chat_message("user"):
                st.markdown(prompt)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –ë–î
            user_message = f"[–ò—Å–ø–æ–ª—å–∑—É–π –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {selected_db}]\n\n{prompt}"
            st.session_state.chat_messages.append(HumanMessage(content=user_message))

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –∞–≥–µ–Ω—Ç–∞
            with st.chat_message("assistant"):
                with st.spinner("–ê–≥–µ–Ω—Ç –¥—É–º–∞–µ—Ç..."):
                    try:
                        result = st.session_state.chat_agent.invoke(
                            {"messages": st.session_state.chat_messages}
                        )

                        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
                        st.session_state.chat_messages = result["messages"]

                        # –°—á–∏—Ç–∞–µ–º –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
                        tool_calls_count = sum(
                            len(msg.tool_calls)
                            if hasattr(msg, "tool_calls") and msg.tool_calls
                            else 0
                            for msg in st.session_state.chat_messages
                        )

                        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç
                        final_response = ""
                        for msg in reversed(st.session_state.chat_messages):
                            if isinstance(msg, AIMessage) and msg.content:
                                if not (hasattr(msg, "tool_calls") and msg.tool_calls):
                                    final_response = msg.content
                                    break

                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ—Ç–≤–µ—Ç
                        st.markdown(final_response)
                        st.caption(f"üîß –í—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {tool_calls_count}")

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                        st.session_state.chat_history.append({
                            "question": prompt,
                            "response": final_response,
                            "tool_calls_count": tool_calls_count,
                            "timestamp": datetime.now().isoformat(),
                            "messages_in_context": len(st.session_state.chat_messages),
                        })

                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞: {e}")
                        logging.exception("–û—à–∏–±–∫–∞ –≤ —á–∞—Ç–µ —Å –∞–≥–µ–Ω—Ç–æ–º")
