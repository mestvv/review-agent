"""Streamlit –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Review Agent."""

import json
import logging
import re
from datetime import datetime
from typing import Optional

import streamlit as st

from src.agent.literature import create_llm
from src.agent.prompts import QA_PROMPT, REVIEW_PROMPT
from src.config import (
    list_existing_dbs,
    list_available_dbs,
    get_articles_subdir,
    CHUNKS_LOG_DIR,
    EXPAND_WINDOW,
    EXPAND_TOP_N,
)
from src.rag import (
    index_all_pdfs,
    clear_database,
    retrieve_chunks,
    retrieve_with_reranking,
    get_neighbor_chunks,
    format_context_with_citations,
    calculate_confidence,
    RetrievedChunk,
    ConfidenceScore,
    ConfidenceLevel,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Review Agent",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded",
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏ (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –≤ –±—É–¥—É—â–µ–º)


def _save_chunks_to_json(
    chunks: list[RetrievedChunk],
    query: str,
    expanded_chunks: Optional[list[RetrievedChunk]] = None,
) -> str:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSON –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É."""
    CHUNKS_LOG_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    filepath = CHUNKS_LOG_DIR / f"chunks_{timestamp}_{safe_query}.json"

    expanded_ids = set()
    if expanded_chunks:
        expanded_ids = {f"{c.file_hash}_{c.chunk_id}" for c in expanded_chunks}

    chunks_data = []
    for chunk in chunks:
        chunk_key = f"{chunk.file_hash}_{chunk.chunk_id}"
        chunks_data.append(
            {
                "chunk_id": chunk.chunk_id,
                "file_name": chunk.file_name,
                "file_hash": chunk.file_hash,
                "page": chunk.page,
                "section": chunk.section,
                "distance": chunk.distance,
                "text": chunk.text,
                "is_expanded": chunk_key in expanded_ids,
            }
        )

    data = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "total_chunks": len(chunks),
        "expanded_chunks_count": len(expanded_chunks) if expanded_chunks else 0,
        "sources": sorted(set(c.file_name for c in chunks)),
        "chunks": chunks_data,
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return str(filepath)


def _format_confidence_dict(confidence: ConfidenceScore) -> dict:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç ConfidenceScore –≤ —Å–ª–æ–≤–∞—Ä—å."""
    level_text = {
        ConfidenceLevel.HIGH: "–í—ã—Å–æ–∫–∞—è",
        ConfidenceLevel.MEDIUM: "–°—Ä–µ–¥–Ω—è—è",
        ConfidenceLevel.LOW: "–ù–∏–∑–∫–∞—è",
        ConfidenceLevel.VERY_LOW: "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
    }

    return {
        "level": level_text.get(confidence.level, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"),
        "level_raw": (
            confidence.level.value
            if hasattr(confidence.level, "value")
            else str(confidence.level)
        ),
        "score": confidence.score,
        "num_sources": confidence.num_sources,
        "num_chunks": confidence.num_chunks,
        "avg_distance": confidence.avg_distance,
        "coverage_by_section": confidence.coverage_by_section,
        "warnings": confidence.warnings,
    }


def _format_confidence_for_prompt(confidence: ConfidenceScore, query_type: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç confidence –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
    level_text = {
        ConfidenceLevel.HIGH: "–í–´–°–û–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –º–æ–∂–Ω–æ –¥–∞–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
        ConfidenceLevel.MEDIUM: "–°–†–ï–î–ù–Ø–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏",
        ConfidenceLevel.LOW: "–ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
        ConfidenceLevel.VERY_LOW: "–û–ß–ï–ù–¨ –ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω, —è–≤–Ω–æ —Å–æ–æ–±—â–∏ –æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
    }

    info = f"""–£—Ä–æ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {level_text.get(confidence.level, '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}
–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources}
–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {confidence.avg_distance:.3f}"""

    if confidence.warnings:
        info += f"\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {'; '.join(confidence.warnings)}"
    return info


def answer_question_web(
    question: str,
    db_name: str,
    n_results: int = 5,
    expand_context: bool = True,
):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
    llm = create_llm()

    initial_chunks, query_type, confidence = retrieve_with_reranking(
        question, db_name, n_results, fetch_multiplier=2
    )

    if not initial_chunks:
        return {
            "success": False,
            "error": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω",
        }

    # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥—è–º–∏
    expanded_chunks = []
    if expand_context:
        seen_ids = {f"{c.file_hash}_{c.chunk_id}" for c in initial_chunks}
        top_n_for_expansion = min(EXPAND_TOP_N, len(initial_chunks))
        for chunk in initial_chunks[:top_n_for_expansion]:
            neighbors = get_neighbor_chunks(
                chunk, db_name, window=EXPAND_WINDOW, query=question
            )
            for n in neighbors:
                key = f"{n.file_hash}_{n.chunk_id}"
                if key not in seen_ids:
                    expanded_chunks.append(n)
                    seen_ids.add(key)
        chunks = (
            initial_chunks[:top_n_for_expansion]
            + expanded_chunks
            + initial_chunks[top_n_for_expansion:]
        )
    else:
        chunks = initial_chunks

    _save_chunks_to_json(chunks, question, expanded_chunks if expand_context else None)

    context = format_context_with_citations(chunks[:10])
    confidence_info = _format_confidence_for_prompt(confidence, query_type)

    response = (QA_PROMPT | llm).invoke(
        {
            "question": question,
            "context": context,
            "confidence_info": confidence_info,
        }
    )

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = {}
    for chunk in chunks:
        if chunk.file_name not in sources:
            sources[chunk.file_name] = {
                "pages": set(),
                "sections": set(),
            }
        sources[chunk.file_name]["pages"].add(chunk.page)
        sources[chunk.file_name]["sections"].add(chunk.section)

    sources_list = []
    for source_name, info in sorted(sources.items()):
        sources_list.append(
            {
                "file_name": source_name,
                "pages": sorted(info["pages"]),
                "sections": sorted(info["sections"]),
            }
        )

    return {
        "success": True,
        "answer": response.content,
        "confidence": _format_confidence_dict(confidence),
        "query_type": query_type,
        "sources": sources_list,
        "chunks_count": len(chunks),
    }


def review_topic_web(
    topic: str,
    db_name: str,
    n_results: int = 15,
):
    """–°–æ–∑–¥–∞–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
    llm = create_llm()
    from src.rag.retriever import detect_query_type

    query_type = detect_query_type(topic)
    all_chunks, query_type, confidence = retrieve_with_reranking(
        topic, db_name, n_results, fetch_multiplier=2
    )

    if not all_chunks:
        return {
            "success": False,
            "error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–∑–æ—Ä–∞",
        }

    confidence = calculate_confidence(all_chunks, query_type)
    _save_chunks_to_json(all_chunks, topic)

    context = format_context_with_citations(all_chunks)

    sources_detail = []
    seen = set()
    for chunk in all_chunks:
        if chunk.file_name not in seen:
            sources_detail.append(f"‚Ä¢ {chunk.file_name}")
            seen.add(chunk.file_name)

    confidence_info = _format_confidence_for_prompt(confidence, query_type)

    response = (REVIEW_PROMPT | llm).invoke(
        {
            "topic": topic,
            "context": context[:8000],
            "sources": "\n".join(sources_detail),
            "confidence_info": confidence_info,
        }
    )

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = {}
    for chunk in all_chunks:
        if chunk.file_name not in sources:
            sources[chunk.file_name] = {
                "pages": set(),
                "sections": set(),
            }
        sources[chunk.file_name]["pages"].add(chunk.page)
        sources[chunk.file_name]["sections"].add(chunk.section)

    sources_list = []
    for source_name, info in sorted(sources.items()):
        sources_list.append(
            {
                "file_name": source_name,
                "pages": sorted(info["pages"]),
                "sections": sorted(info["sections"]),
            }
        )

    return {
        "success": True,
        "review": response.content,
        "confidence": _format_confidence_dict(confidence),
        "query_type": query_type,
        "sources": sources_list,
        "chunks_count": len(all_chunks),
    }


def get_stats_dict(db_name: Optional[str] = None) -> dict:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è."""
    from src.rag.indexer import _get_collection

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ –º–æ–¥—É–ª—è
    import src.config as config

    if db_name:
        collection = _get_collection(db_name)
        total = collection.count()
        if total == 0:
            return {"db_name": db_name, "total": 0, "files": {}, "sections": {}}

        results = collection.get(include=["metadatas"])
        metadatas = results["metadatas"]

        files = {}
        sections = {}
        for metadata in metadatas:
            fname = metadata.get("file_name", "unknown")
            section = metadata.get("section", "unknown")
            files[fname] = files.get(fname, 0) + 1
            sections[section] = sections.get(section, 0) + 1

        return {
            "db_name": db_name,
            "total": total,
            "files": files,
            "sections": sections,
        }
    else:
        existing_dbs_list = config.list_existing_dbs()
        stats_dict = {}
        total_chunks = 0
        for db_item in existing_dbs_list:
            collection = _get_collection(db_item)
            count = collection.count()
            total_chunks += count
            stats_dict[db_item] = {"total": count}

        return {
            "all_dbs": stats_dict,
            "total_chunks": total_chunks,
        }


# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ë–î
with st.sidebar:
    st.title("üìö Review Agent")
    st.markdown("---")

    st.subheader("–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")

    existing_dbs = list_existing_dbs()
    available_dbs = list_available_dbs()

    if existing_dbs:
        selected_db = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö",
            existing_dbs,
            key="selected_db",
        )
    else:
        st.warning("–ù–µ—Ç –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö")
        selected_db = None

    st.markdown("---")

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ë–î
    st.subheader("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")

    with st.expander("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"):
        if st.button("–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É", use_container_width=True):
            if selected_db:
                stats = get_stats_dict(selected_db)
                st.json(stats)
            else:
                stats = get_stats_dict()
                st.json(stats)

    with st.expander("üîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        db_to_index = st.selectbox(
            "–ë–∞–∑–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏",
            [None] + available_dbs,
            key="db_to_index",
        )
        if st.button("–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å", use_container_width=True):
            if db_to_index:
                with st.spinner(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã '{db_to_index}'..."):
                    try:
                        index_all_pdfs(db_name=db_to_index)
                        st.success(f"–ë–∞–∑–∞ '{db_to_index}' –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞: {e}")
            else:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

    with st.expander("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ"):
        db_to_delete = st.selectbox(
            "–ë–∞–∑–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è",
            [None] + existing_dbs,
            key="db_to_delete",
        )
        if st.button("–£–¥–∞–ª–∏—Ç—å", use_container_width=True, type="primary"):
            if db_to_delete:
                try:
                    clear_database(db_name=db_to_delete)
                    st.success(f"–ë–∞–∑–∞ '{db_to_delete}' —É–¥–∞–ª–µ–Ω–∞!")
                    st.rerun()
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞: {e}")
            else:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")

# –ì–ª–∞–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
st.title("üìö Review Agent")
st.markdown("RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ–±–∑–æ—Ä–æ–≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")

if not selected_db:
    st.info("üëà –í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é")
    st.markdown("---")
    st.subheader("–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    if available_dbs:
        for db in available_dbs:
            articles_subdir = get_articles_subdir(db)
            pdf_count = len(list(articles_subdir.glob("*.pdf")))
            md_count = len(list(articles_subdir.glob("*.md")))
            docx_count = len(list(articles_subdir.glob("*.docx")))
            st.write(f"**{db}**: {pdf_count} PDF, {md_count} MD, {docx_count} DOCX")
    else:
        st.warning(
            "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π. –°–æ–∑–¥–∞–π—Ç–µ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø–∞–ø–∫–µ `articles/`"
        )
else:
    # –í–∫–ª–∞–¥–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
    tab1, tab2, tab3 = st.tabs(
        ["üí¨ –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å", "üìù –û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "üîç –ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤"]
    )

    with tab1:
        st.subheader("–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")

        question = st.text_area(
            "–í–∞—à –≤–æ–ø—Ä–æ—Å",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –° –∫–∞–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ?",
            height=100,
        )

        col1, col2 = st.columns(2)
        with col1:
            n_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤", 1, 20, 5)
        with col2:
            expand_context = st.checkbox("–†–∞—Å—à–∏—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç", value=True)

        if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–æ–ø—Ä–æ—Å", type="primary", use_container_width=True):
            if question:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞..."):
                    result = answer_question_web(
                        question,
                        selected_db,
                        n_results=n_results,
                        expand_context=expand_context,
                    )

                    if result["success"]:
                        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                        confidence = result["confidence"]
                        level_colors = {
                            "–í—ã—Å–æ–∫–∞—è": "üü¢",
                            "–°—Ä–µ–¥–Ω—è—è": "üü°",
                            "–ù–∏–∑–∫–∞—è": "üü†",
                            "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è": "üî¥",
                        }
                        icon = level_colors.get(confidence["level"], "‚ö™")

                        st.markdown("---")
                        st.markdown(f"### {icon} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence['level']}")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤", confidence["num_sources"])
                        with col2:
                            st.metric("–ß–∞–Ω–∫–æ–≤", confidence["num_chunks"])
                        with col3:
                            st.metric(
                                "–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è", f"{confidence['avg_distance']:.3f}"
                            )

                        # –û—Ç–≤–µ—Ç
                        st.markdown("---")
                        st.markdown("### –û—Ç–≤–µ—Ç")
                        st.markdown(result["answer"])

                        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                        if result["sources"]:
                            st.markdown("---")
                            st.markdown("### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
                            for source in result["sources"]:
                                pages_str = ", ".join(map(str, source["pages"]))
                                sections_str = (
                                    ", ".join(source["sections"])
                                    if source["sections"]
                                    else "‚Äî"
                                )
                                with st.expander(f"üìÑ {source['file_name']}"):
                                    st.write(f"**–°—Ç—Ä–∞–Ω–∏—Ü—ã:** {pages_str}")
                                    st.write(f"**–°–µ–∫—Ü–∏–∏:** {sections_str}")
                    else:
                        st.error(result.get("error", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞"))
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")

    with tab2:
        st.subheader("–û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")

        topic = st.text_area(
            "–¢–µ–º–∞ –æ–±–∑–æ—Ä–∞",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤–µ—á–Ω–æ–π –º–µ—Ä–∑–ª–æ—Ç—ã –≤ –†–æ—Å—Å–∏–∏",
            height=100,
        )

        n_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤", 5, 30, 15)

        if st.button("–°–æ–∑–¥–∞—Ç—å –æ–±–∑–æ—Ä", type="primary", use_container_width=True):
            if topic:
                with st.spinner("–°–æ–∑–¥–∞–Ω–∏–µ –æ–±–∑–æ—Ä–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã..."):
                    result = review_topic_web(
                        topic,
                        selected_db,
                        n_results=n_results,
                    )

                    if result["success"]:
                        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                        confidence = result["confidence"]
                        level_colors = {
                            "–í—ã—Å–æ–∫–∞—è": "üü¢",
                            "–°—Ä–µ–¥–Ω—è—è": "üü°",
                            "–ù–∏–∑–∫–∞—è": "üü†",
                            "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è": "üî¥",
                        }
                        icon = level_colors.get(confidence["level"], "‚ö™")

                        st.markdown("---")
                        st.markdown(f"### {icon} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence['level']}")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤", confidence["num_sources"])
                        with col2:
                            st.metric("–ß–∞–Ω–∫–æ–≤", confidence["num_chunks"])
                        with col3:
                            st.metric(
                                "–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è", f"{confidence['avg_distance']:.3f}"
                            )

                        # –û–±–∑–æ—Ä
                        st.markdown("---")
                        st.markdown("### –û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")
                        st.markdown(result["review"])

                        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                        if result["sources"]:
                            st.markdown("---")
                            st.markdown("### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
                            for source in result["sources"]:
                                pages_str = ", ".join(map(str, source["pages"]))
                                sections_str = (
                                    ", ".join(source["sections"])
                                    if source["sections"]
                                    else "‚Äî"
                                )
                                with st.expander(f"üìÑ {source['file_name']}"):
                                    st.write(f"**–°—Ç—Ä–∞–Ω–∏—Ü—ã:** {pages_str}")
                                    st.write(f"**–°–µ–∫—Ü–∏–∏:** {sections_str}")
                    else:
                        st.error(result.get("error", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞"))
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –æ–±–∑–æ—Ä–∞")

    with tab3:
        st.subheader("–ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤")

        query = st.text_input(
            "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å",
            placeholder="–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞",
        )

        col1, col2 = st.columns(2)
        with col1:
            n_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", 1, 50, 10)
        with col2:
            section = st.selectbox(
                "–§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏",
                [
                    None,
                    "abstract",
                    "introduction",
                    "methods",
                    "results",
                    "discussion",
                    "conclusion",
                ],
            )

        if st.button("–ü–æ–∏—Å–∫", type="primary", use_container_width=True):
            if query:
                with st.spinner("–ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤..."):
                    chunks = retrieve_chunks(query, selected_db, n_results, section)

                    if chunks:
                        st.markdown(f"### –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
                        st.markdown("---")

                        for i, chunk in enumerate(chunks, 1):
                            with st.expander(
                                f"–ß–∞–Ω–∫ {i}: {chunk.file_name} (—Å—Ç—Ä. {chunk.page}, —Å–µ–∫—Ü–∏—è: {chunk.section}, dist: {chunk.distance:.3f})"
                            ):
                                st.text(chunk.text)
                    else:
                        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
