"""–ü–æ–∏—Å–∫ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ RAG –±–∞–∑—ã."""

import logging
from dataclasses import dataclass, field
from enum import Enum
from typing import Optional

import chromadb
import numpy as np
from sentence_transformers import SentenceTransformer

from src.config import (
    COLLECTION_NAME,
    SENTENCE_TRANSFORMER_MODEL,
    SECTION_WEIGHTS,
    QUERY_TYPE_KEYWORDS,
    CONFIDENCE_THRESHOLDS,
    MIN_CHUNKS_FOR_CONFIDENT_ANSWER,
    RERANKER_MODEL,
    RERANKER_TOP_K,
    USE_RERANKER,
    INITIAL_FETCH_COUNT,
    get_db_path,
)

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
_embedding_model = None
_clients = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ë–î
_collections = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ë–î
_reranker_model = None


def _get_embedding_model():
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)
    return _embedding_model


def _get_collection(db_name: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –ë–î."""
    global _clients, _collections
    if db_name not in _collections:
        db_path = get_db_path(db_name)
        _clients[db_name] = chromadb.PersistentClient(path=db_path)
        _collections[db_name] = _clients[db_name].get_or_create_collection(
            name=COLLECTION_NAME
        )
    return _collections[db_name]


def _get_reranker():
    """Lazy loading reranker –º–æ–¥–µ–ª–∏."""
    global _reranker_model
    if not USE_RERANKER or not RERANKER_MODEL:
        return None
    if _reranker_model is None:
        try:
            from sentence_transformers import CrossEncoder

            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ reranker: {RERANKER_MODEL}")
            _reranker_model = CrossEncoder(RERANKER_MODEL, trust_remote_code=True)
            logger.info("‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker: {e}")
            return None
    return _reranker_model


class ConfidenceLevel(Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    VERY_LOW = "very_low"


@dataclass
class RetrievedChunk:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —á–∞–Ω–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""

    text: str
    file_name: str
    file_hash: str
    chunk_id: int
    page: int
    section: str
    distance: float
    reranked_score: float = 0.0

    def citation(self) -> str:
        return f"[{self.file_name}, —Å—Ç—Ä. {self.page}]"

    def full_citation(self) -> str:
        return f"{self.file_name} (—Å—Ç—Ä. {self.page}, {self.section})"


@dataclass
class ConfidenceScore:
    """–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞."""

    level: ConfidenceLevel
    score: float
    avg_distance: float
    min_distance: float
    max_distance: float
    num_chunks: int
    num_sources: int
    coverage_by_section: dict = field(default_factory=dict)
    warnings: list = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "level": self.level.value,
            "score": round(self.score, 3),
            "avg_distance": round(self.avg_distance, 3),
            "min_distance": round(self.min_distance, 3),
            "max_distance": round(self.max_distance, 3),
            "num_chunks": self.num_chunks,
            "num_sources": self.num_sources,
            "coverage_by_section": self.coverage_by_section,
            "warnings": self.warnings,
        }


def detect_query_type(query: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
    query_lower = query.lower()
    type_scores = {}
    for query_type, keywords in QUERY_TYPE_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in query_lower)
        if score > 0:
            type_scores[query_type] = score
    if not type_scores:
        return "default"
    return max(type_scores, key=type_scores.get)


def retrieve_chunks(
    query: str,
    db_name: str,
    n_results: int = 5,
    section_filter: Optional[str] = None,
) -> list[RetrievedChunk]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –±–∞–∑—ã.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        section_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ –ë–î '{db_name}': '{query[:60]}...'")

    model = _get_embedding_model()
    collection = _get_collection(db_name)

    embedding = model.encode([query]).tolist()
    where_filter = {"section": section_filter} if section_filter else None

    results = collection.query(
        query_embeddings=embedding,
        n_results=n_results,
        where=where_filter,
        include=["documents", "metadatas", "distances"],
    )

    chunks = []
    for doc, meta, dist in zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0],
    ):
        chunks.append(
            RetrievedChunk(
                text=doc,
                file_name=meta.get("file_name", "unknown"),
                file_hash=meta.get("file_hash", ""),
                chunk_id=meta.get("chunk_id", 0),
                page=meta.get("page", 0),
                section=meta.get("section", "unknown"),
                distance=dist,
            )
        )

    sources = sorted(set(c.file_name for c in chunks))
    logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    return chunks


def get_neighbor_chunks(
    chunk: RetrievedChunk, db_name: str, window: int = 1, query: Optional[str] = None
) -> list[RetrievedChunk]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    
    Args:
        chunk: –ß–∞–Ω–∫, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –∏—â–µ–º —Å–æ—Å–µ–¥–µ–π
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        window: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã)
        query: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ distance
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤
    """
    collection = _get_collection(db_name)
    neighbor_ids = [
        f"{chunk.file_hash}_{chunk.chunk_id + offset}"
        for offset in range(-window, window + 1)
    ]

    results = collection.get(ids=neighbor_ids, include=["documents", "metadatas"])

    neighbors = []
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω query, –≤—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ distance –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞
    query_embedding = None
    if query:
        model = _get_embedding_model()
        query_embedding = model.encode([query])[0]

    for doc, meta in zip(results["documents"], results["metadatas"]):
        if doc and meta:
            chunk_id_val = meta.get("chunk_id", 0)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ distance, –µ—Å–ª–∏ query –ø–µ—Ä–µ–¥–∞–Ω
            if query and query_embedding is not None:
                model = _get_embedding_model()
                doc_embedding = model.encode([doc])[0]
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = float(
                    np.dot(query_embedding, doc_embedding)
                    / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))
                )
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ distance (1 - similarity –¥–ª—è ChromaDB)
                distance = 1.0 - similarity
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è expanded chunks –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ distance
                # —á—Ç–æ–±—ã –æ–Ω–∏ –±—ã–ª–∏ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ –ø–æ distance
                distance = 999.0
            
            neighbors.append(
                RetrievedChunk(
                    text=doc,
                    file_name=meta.get("file_name", "unknown"),
                    file_hash=meta.get("file_hash", ""),
                    chunk_id=chunk_id_val,
                    page=meta.get("page", 0),
                    section=meta.get("section", "unknown"),
                    distance=distance,
                )
            )
    return sorted(neighbors, key=lambda c: c.chunk_id)


def format_context_with_citations(chunks: list[RetrievedChunk]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏."""
    formatted = []
    for i, chunk in enumerate(chunks, 1):
        formatted.append(
            f"[{i}] {chunk.citation()}\n–°–µ–∫—Ü–∏—è: {chunk.section}\n---\n{chunk.text}\n"
        )
    return "\n".join(formatted)


def rerank_chunks_with_model(
    query: str,
    chunks: list[RetrievedChunk],
    top_k: Optional[int] = None,
) -> list[RetrievedChunk]:
    """–†–µ-—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é CrossEncoder."""
    reranker = _get_reranker()
    if reranker is None or not chunks:
        return chunks

    documents = [chunk.text for chunk in chunks]
    try:
        results = reranker.rank(
            query, documents, return_documents=False, top_k=top_k or len(chunks)
        )
        reranked_chunks = []
        for result in results:
            idx = result["corpus_id"]
            chunk = chunks[idx]
            chunk.reranked_score = result["score"]
            reranked_chunks.append(chunk)
        logger.info(f"üéØ Reranker: —Ç–æ–ø score={reranked_chunks[0].reranked_score:.3f}")
        return reranked_chunks
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ reranker: {e}")
        return chunks


def rerank_chunks_heuristic(
    chunks: list[RetrievedChunk], query_type: str
) -> list[RetrievedChunk]:
    """–†–µ-—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π —Å –≤–µ—Å–∞–º–∏ —Å–µ–∫—Ü–∏–π."""
    weights = SECTION_WEIGHTS.get(query_type, SECTION_WEIGHTS["default"])
    for chunk in chunks:
        section_weight = weights.get(chunk.section, 1.0)
        chunk.reranked_score = (1.0 - chunk.distance) * section_weight
    return sorted(chunks, key=lambda c: c.reranked_score, reverse=True)


def rerank_chunks(
    query: str,
    chunks: list[RetrievedChunk],
    query_type: str,
    top_k: Optional[int] = None,
) -> list[RetrievedChunk]:
    """–†–µ-—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ (ML –º–æ–¥–µ–ª—å –∏–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)."""
    if not chunks:
        return chunks

    reranker = _get_reranker()
    if reranker is not None:
        reranked = rerank_chunks_with_model(query, chunks, top_k)
        if reranked and reranked[0].reranked_score > 0:
            return reranked[:top_k] if top_k else reranked

    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É —Å –≤–µ—Å–∞–º–∏ —Å–µ–∫—Ü–∏–π")
    reranked = rerank_chunks_heuristic(chunks, query_type)
    return reranked[:top_k] if top_k else reranked


def calculate_confidence(
    chunks: list[RetrievedChunk], query_type: str
) -> ConfidenceScore:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏."""
    warnings = []

    if not chunks:
        return ConfidenceScore(
            level=ConfidenceLevel.VERY_LOW,
            score=0.0,
            avg_distance=1.0,
            min_distance=1.0,
            max_distance=1.0,
            num_chunks=0,
            num_sources=0,
            warnings=["–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"],
        )

    distances = [c.distance for c in chunks]
    avg_distance = sum(distances) / len(distances)
    min_distance = min(distances)
    max_distance = max(distances)
    sources = set(c.file_name for c in chunks)
    num_sources = len(sources)

    section_coverage = {}
    for chunk in chunks:
        section_coverage[chunk.section] = section_coverage.get(chunk.section, 0) + 1

    if avg_distance < CONFIDENCE_THRESHOLDS["high"]:
        level, base_score = ConfidenceLevel.HIGH, 0.9
    elif avg_distance < CONFIDENCE_THRESHOLDS["medium"]:
        level, base_score = ConfidenceLevel.MEDIUM, 0.7
    elif avg_distance < CONFIDENCE_THRESHOLDS["low"]:
        level, base_score = ConfidenceLevel.LOW, 0.5
    else:
        level, base_score = ConfidenceLevel.VERY_LOW, 0.3

    score = base_score

    if len(chunks) >= MIN_CHUNKS_FOR_CONFIDENT_ANSWER:
        score += 0.05
    else:
        warnings.append(f"–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(chunks)} —á–∞–Ω–∫–æ–≤")
        score -= 0.1

    if num_sources >= 2:
        score += 0.05
    else:
        warnings.append("–û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–¥–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ")

    distance_spread = max_distance - min_distance
    if distance_spread > 0.4:
        warnings.append(f"–í—ã—Å–æ–∫–∏–π —Ä–∞–∑–±—Ä–æ—Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {distance_spread:.2f}")
        score -= 0.05

    important_sections = {
        "results": ["results", "conclusion", "discussion"],
        "methods": ["methods"],
        "definitions": ["introduction", "methods"],
        "overview": ["abstract", "introduction", "conclusion"],
    }
    if query_type in important_sections:
        covered = [s for s in important_sections[query_type] if s in section_coverage]
        if not covered:
            warnings.append(f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–µ–∫—Ü–∏–π –¥–ª—è '{query_type}'")
            score -= 0.1

    score = max(0.0, min(1.0, score))

    return ConfidenceScore(
        level=level,
        score=score,
        avg_distance=avg_distance,
        min_distance=min_distance,
        max_distance=max_distance,
        num_chunks=len(chunks),
        num_sources=num_sources,
        coverage_by_section=section_coverage,
        warnings=warnings,
    )


def retrieve_with_reranking(
    query: str,
    db_name: str,
    n_results: int = 5,
    section_filter: Optional[str] = None,
    fetch_multiplier: int = 3,
) -> tuple[list[RetrievedChunk], str, ConfidenceScore]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∞–Ω–∫–∏ —Å re-ranking.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ re-ranking
        section_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        fetch_multiplier: –ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∏–∑ —Å–ø–∏—Å–∫–∞ —á–∞–Ω–∫–æ–≤, —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    """
    query_type = detect_query_type(query)
    logger.info(f"üéØ –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º INITIAL_FETCH_COUNT –∫–∞–∫ –º–∏–Ω–∏–º—É–º, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
    fetch_count = max(n_results * fetch_multiplier, RERANKER_TOP_K, INITIAL_FETCH_COUNT)
    initial_chunks = retrieve_chunks(query, db_name, fetch_count, section_filter)

    if not initial_chunks:
        return [], query_type, calculate_confidence([], query_type)

    top_chunks = rerank_chunks(query, initial_chunks, query_type, top_k=n_results)
    confidence = calculate_confidence(top_chunks, query_type)

    changes = sum(
        1
        for i, c in enumerate(top_chunks)
        if i < len(initial_chunks) and c != initial_chunks[i]
    )
    logger.info(f"üìä Re-ranking: –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Ç–æ–ø-{n_results}: {changes}")

    return top_chunks, query_type, confidence
