"""Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ RAG-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Any

import numpy as np
from langchain_core.tools import tool

from src.config import list_existing_dbs, CHUNKS_LOG_DIR
from src.rag.retriever import (
    retrieve_with_reranking,
    retrieve_chunks,
    get_neighbor_chunks,
    format_context_with_citations,
    RetrievedChunk,
    ConfidenceLevel,
    ConfidenceScore,
)

logger = logging.getLogger(__name__)

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞµÑÑĞ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
_current_agent_session_dir: Optional[Path] = None


def _get_agent_session_dir() -> Path:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑĞµÑÑĞ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."""
    global _current_agent_session_dir
    if _current_agent_session_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        _current_agent_session_dir = CHUNKS_LOG_DIR / f"{timestamp}_agent"
        _current_agent_session_dir.mkdir(parents=True, exist_ok=True)
        logger.info(
            "ğŸ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: %s", _current_agent_session_dir
        )
    return _current_agent_session_dir


def reset_agent_session_dir() -> None:
    """Ğ¡Ğ±Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° (Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞµÑÑĞ¸Ğ¸)."""
    global _current_agent_session_dir
    _current_agent_session_dir = None


def _convert_numpy_types(obj: Any) -> Any:
    """Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ numpy Ñ‚Ğ¸Ğ¿Ñ‹ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Python Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ JSON ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.

    Args:
        obj: ĞĞ±ÑŠĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

    Returns:
        ĞĞ±ÑŠĞµĞºÑ‚ Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸
    """
    if isinstance(obj, (np.integer, np.floating)):
        return float(obj) if isinstance(obj, np.floating) else int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: _convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [_convert_numpy_types(item) for item in obj]
    elif isinstance(obj, set):
        return {_convert_numpy_types(item) for item in obj}
    else:
        return obj


def _save_agent_chunks(
    chunks: list[RetrievedChunk],
    query: str,
    tool_name: str,
    confidence: Optional[ConfidenceScore] = None,
    query_type: Optional[str] = None,
    expanded_chunks: Optional[list[RetrievedChunk]] = None,
) -> str:
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‡Ğ°Ğ½ĞºĞ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ² JSON Ñ„Ğ°Ğ¹Ğ».

    Args:
        chunks: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
        query: ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
        tool_name: Ğ˜Ğ¼Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°
        confidence: ĞÑ†ĞµĞ½ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
        query_type: Ğ¢Ğ¸Ğ¿ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        expanded_chunks: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)

    Returns:
        ĞŸÑƒÑ‚ÑŒ Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°Ğ¹Ğ»Ñƒ
    """
    session_dir = _get_agent_session_dir()

    # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¸Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸Ğ· Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    timestamp = datetime.now().strftime("%H%M%S")
    filepath = session_dir / f"{timestamp}_{safe_query}.json"

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ĞºĞ°ĞºĞ¸Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ñ‹
    expanded_ids = set()
    if expanded_chunks:
        expanded_ids = {f"{c.file_hash}_{c.chunk_id}" for c in expanded_chunks}

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ°Ñ…
    chunks_data = []
    for chunk in chunks:
        chunk_key = f"{chunk.file_hash}_{chunk.chunk_id}"
        chunks_data.append(
            {
                "chunk_id": chunk.chunk_id,
                "file_name": chunk.file_name,
                "file_hash": chunk.file_hash,
                "page": chunk.page,
                "section": chunk.section,
                "distance": chunk.distance,
                "reranked_score": chunk.reranked_score,
                "text": chunk.text,
                "is_expanded": chunk_key in expanded_ids,
            }
        )

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    data = {
        "tool_name": tool_name,
        "query": query,
        "query_type": query_type,
        "timestamp": datetime.now().isoformat(),
        "total_chunks": len(chunks),
        "expanded_chunks_count": len(expanded_chunks) if expanded_chunks else 0,
        "sources": sorted(set(c.file_name for c in chunks)),
        "chunks": chunks_data,
    }

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ confidence ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
    if confidence:
        data["confidence"] = {
            "level": confidence.level.value,
            "score": confidence.score,
            "avg_distance": confidence.avg_distance,
            "min_distance": confidence.min_distance,
            "max_distance": confidence.max_distance,
            "num_chunks": confidence.num_chunks,
            "num_sources": confidence.num_sources,
            "coverage_by_section": confidence.coverage_by_section,
            "warnings": confidence.warnings,
        }

    # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ numpy Ñ‚Ğ¸Ğ¿Ñ‹ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Python Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ JSON ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
    data = _convert_numpy_types(data)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ñ„Ğ°Ğ¹Ğ»
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    logger.info("ğŸ’¾ Ğ§Ğ°Ğ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: %s", filepath)
    return str(filepath)


@tool
def search_vector_db(
    query: str,
    db_name: str,
    n_results: int = 5,
    expand_context: bool = True,
    section_filter: Optional[str] = None,
) -> str:
    """ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹.

    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑÑ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ….
    Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ ÑĞµĞºÑ†Ğ¸Ğ¹.

    Args:
        query: ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.
               Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾.
        db_name: Ğ˜Ğ¼Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'climate').
                 Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ‘Ğ” Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· list_available_databases.
        n_results: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 5).
        expand_context: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ True).
        section_filter: Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ ÑĞµĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾).
                       Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ: 'abstract', 'introduction', 'methods',
                       'results', 'discussion', 'conclusion'.

    Returns:
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ± ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.
    """
    logger.info("ğŸ”§ Tool search_vector_db: query='%s...', db='%s'", query[:50], db_name)

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ‘Ğ”
    existing_dbs = list_existing_dbs()
    if db_name not in existing_dbs:
        available = ", ".join(existing_dbs) if existing_dbs else "Ğ½ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ‘Ğ”"
        return f"âŒ Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… '{db_name}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ‘Ğ”: {available}"

    # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸ Ñ Ñ€ĞµÑ€Ğ°Ğ½ĞºĞ¸Ğ½Ğ³Ğ¾Ğ¼
    initial_chunks, query_type, confidence = retrieve_with_reranking(
        query=query,
        db_name=db_name,
        n_results=n_results,
        section_filter=section_filter,
        fetch_multiplier=3,
    )

    if not initial_chunks:
        return f"âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: '{query}'"

    # Ğ Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸, ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
    chunks = initial_chunks
    if expand_context:
        seen_ids = {f"{c.file_hash}_{c.chunk_id}" for c in initial_chunks}
        expanded_chunks = []

        # Ğ Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼ Ñ‚Ğ¾Ğ¿-3 Ñ‡Ğ°Ğ½ĞºĞ°
        for chunk in initial_chunks[:3]:
            neighbors = get_neighbor_chunks(chunk, db_name, window=1, query=query)
            for n in neighbors:
                key = f"{n.file_hash}_{n.chunk_id}"
                if key not in seen_ids:
                    expanded_chunks.append(n)
                    seen_ids.add(key)

        chunks = initial_chunks + expanded_chunks

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
    context = format_context_with_citations(chunks[:10])

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ² Ğ»Ğ¾Ğ³
    _save_agent_chunks(
        chunks=chunks,
        query=query,
        tool_name="search_vector_db",
        confidence=confidence,
        query_type=query_type,
        expanded_chunks=expanded_chunks if expand_context else None,
    )

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
    level_text = {
        ConfidenceLevel.HIGH: "Ğ’Ğ«Ğ¡ĞĞšĞĞ¯",
        ConfidenceLevel.MEDIUM: "Ğ¡Ğ Ğ•Ğ”ĞĞ¯Ğ¯",
        ConfidenceLevel.LOW: "ĞĞ˜Ğ—ĞšĞĞ¯",
        ConfidenceLevel.VERY_LOW: "ĞĞ§Ğ•ĞĞ¬ ĞĞ˜Ğ—ĞšĞĞ¯",
    }.get(confidence.level, "ĞĞ•Ğ˜Ğ—Ğ’Ğ•Ğ¡Ğ¢ĞĞ")

    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸
    sources = sorted(set(c.file_name for c in chunks))

    result = f"""ğŸ“Š Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ĞŸĞĞ˜Ğ¡ĞšĞ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: {query}
Ğ¢Ğ¸Ğ¿ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: {query_type}
Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ: {level_text} (score: {confidence.score:.2f})
Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ: {confidence.avg_distance:.3f}
ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²: {len(chunks)}
Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: {len(sources)}
"""

    if confidence.warnings:
        result += f"âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ: {'; '.join(confidence.warnings)}\n"

    result += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞĞ«Ğ• Ğ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞ˜:
{chr(10).join(f'â€¢ {s}' for s in sources)}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ĞĞĞ™Ğ”Ğ•ĞĞĞ«Ğ• Ğ¤Ğ ĞĞ“ĞœĞ•ĞĞ¢Ğ«:

{context}
"""

    logger.info("âœ… Tool search_vector_db: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ %d Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²", len(chunks))
    return result


@tool
def list_available_databases() -> str:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.

    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑÑ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ ĞºĞ°ĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹
    Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹.

    Returns:
        Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼.
    """
    logger.info("ğŸ”§ Tool list_available_databases called")

    existing_dbs = list_existing_dbs()

    if not existing_dbs:
        return (
            "âŒ ĞĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸."
        )

    result = "ğŸ“š Ğ”ĞĞ¡Ğ¢Ğ£ĞŸĞĞ«Ğ• Ğ‘ĞĞ—Ğ« Ğ”ĞĞĞĞ«Ğ¥:\n\n"
    for db_name in existing_dbs:
        result += f"â€¢ {db_name}\n"

    result += (
        "\nĞ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ‘Ğ” Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğµ db_name Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° search_vector_db."
    )

    logger.info("âœ… Tool list_available_databases: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ %d Ğ‘Ğ”", len(existing_dbs))
    return result


@tool
def search_by_section(
    query: str,
    db_name: str,
    sections: list[str],
    n_results_per_section: int = 3,
) -> str:
    """ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞºÑ†Ğ¸ÑÑ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾.

    ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ ÑÑ‚Ğ°Ñ‚ĞµĞ¹:
    Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ˜ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾.

    Args:
        query: ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.
        db_name: Ğ˜Ğ¼Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.
        sections: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞµĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.
                 Ğ”Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ: 'abstract', 'introduction', 'methods',
                 'results', 'discussion', 'conclusion'.
        n_results_per_section: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞµĞºÑ†Ğ¸Ñ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 3).

    Returns:
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµĞºÑ†Ğ¸Ğ¹.
    """
    logger.info(
        "ğŸ”§ Tool search_by_section: query='%s...', sections=%s", query[:50], sections
    )

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ‘Ğ”
    existing_dbs = list_existing_dbs()
    if db_name not in existing_dbs:
        available = ", ".join(existing_dbs) if existing_dbs else "Ğ½ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ‘Ğ”"
        return f"âŒ Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… '{db_name}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ‘Ğ”: {available}"

    all_chunks: list[RetrievedChunk] = []
    section_results = {}

    for section in sections:
        chunks = retrieve_chunks(
            query=query,
            db_name=db_name,
            n_results=n_results_per_section,
            section_filter=section,
        )
        section_results[section] = len(chunks)
        all_chunks.extend(chunks)

    if not all_chunks:
        return f"âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: '{query}'"

    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ distance
    all_chunks.sort(key=lambda c: c.distance)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ² Ğ»Ğ¾Ğ³
    _save_agent_chunks(
        chunks=all_chunks,
        query=query,
        tool_name="search_by_section",
        query_type=f"sections:{','.join(sections)}",
    )

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    context = format_context_with_citations(all_chunks[:10])
    sources = sorted(set(c.file_name for c in all_chunks))

    result = f"""ğŸ“Š Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ĞŸĞĞ˜Ğ¡ĞšĞ ĞŸĞ Ğ¡Ğ•ĞšĞ¦Ğ˜Ğ¯Ğœ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: {query}
Ğ¡ĞµĞºÑ†Ğ¸Ğ¸: {', '.join(sections)}
ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ğ¾ ÑĞµĞºÑ†Ğ¸ÑĞ¼: {section_results}
Ğ’ÑĞµĞ³Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²: {len(all_chunks)}
Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: {len(sources)}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞĞ«Ğ• Ğ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞ˜:
{chr(10).join(f'â€¢ {s}' for s in sources)}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ĞĞĞ™Ğ”Ğ•ĞĞĞ«Ğ• Ğ¤Ğ ĞĞ“ĞœĞ•ĞĞ¢Ğ«:

{context}
"""

    logger.info("âœ… Tool search_by_section: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ %d Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²", len(all_chunks))
    return result


# Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑĞµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
ALL_TOOLS = [
    search_vector_db,
    list_available_databases,
    search_by_section,
]

# Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
__all__ = [
    "ALL_TOOLS",
    "search_vector_db",
    "list_available_databases",
    "search_by_section",
    "reset_agent_session_dir",
]
