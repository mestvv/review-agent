"""–ê–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π."""

import json
import re
import logging
from datetime import datetime
from typing import Optional

from langchain_openai import ChatOpenAI
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.rule import Rule
from rich.table import Table

from src.config import (
    CHUNKS_LOG_DIR,
    RESPONSES_LOG_DIR,
    RESULTS_DIR,
    LLM_MODEL,
    LLM_BASE_URL,
    LLM_API_KEY,
    LLM_TEMPERATURE,
    EXPAND_WINDOW,
    EXPAND_TOP_N,
)
from src.rag.retriever import (
    retrieve_chunks,
    retrieve_with_reranking,
    get_neighbor_chunks,
    format_context_with_citations,
    calculate_confidence,
    detect_query_type,
    RetrievedChunk,
    ConfidenceScore,
    ConfidenceLevel,
)
from src.agent.prompts import QA_PROMPT, REVIEW_PROMPT

logger = logging.getLogger(__name__)
console = Console()


def create_llm():
    """–°–æ–∑–¥–∞—ë—Ç LLM –∫–ª–∏–µ–Ω—Ç."""
    return ChatOpenAI(
        model=LLM_MODEL,
        base_url=LLM_BASE_URL,
        api_key=LLM_API_KEY,
        temperature=LLM_TEMPERATURE,
        extra_body={"thinking": {"type": "enabled"}},
    )


def _save_chunks_to_json(
    chunks: list[RetrievedChunk],
    query: str,
    expanded_chunks: Optional[list[RetrievedChunk]] = None,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ JSON."""
    CHUNKS_LOG_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    filepath = CHUNKS_LOG_DIR / f"chunks_{timestamp}_{safe_query}.json"

    expanded_ids = set()
    if expanded_chunks:
        expanded_ids = {f"{c.file_hash}_{c.chunk_id}" for c in expanded_chunks}

    chunks_data = []
    for chunk in chunks:
        chunk_key = f"{chunk.file_hash}_{chunk.chunk_id}"
        chunks_data.append(
            {
                "chunk_id": chunk.chunk_id,
                "file_name": chunk.file_name,
                "file_hash": chunk.file_hash,
                "page": chunk.page,
                "section": chunk.section,
                "distance": chunk.distance,
                "text": chunk.text,
                "is_expanded": chunk_key in expanded_ids,
            }
        )

    data = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "total_chunks": len(chunks),
        "expanded_chunks_count": len(expanded_chunks) if expanded_chunks else 0,
        "sources": sorted(set(c.file_name for c in chunks)),
        "chunks": chunks_data,
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"üíæ –ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def _save_response_to_json(query, response) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç LLM –≤ JSON."""
    RESPONSES_LOG_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if isinstance(query, dict):
        query_str = query.get("question") or query.get("topic") or str(query)
    else:
        query_str = query

    safe_query = re.sub(r"[^\w\s-]", "", str(query_str)[:50]).strip().replace(" ", "_")
    filepath = RESPONSES_LOG_DIR / f"response_{timestamp}_{safe_query}.json"

    response_dict = response.dict() if hasattr(response, "dict") else str(response)

    data = {
        "query": query,
        "response": response_dict,
        "timestamp": datetime.now().isoformat(),
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)
    logger.info(f"üíæ –û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {filepath}")


def _save_to_markdown(
    query: str,
    response_content: str,
    chunks: list[RetrievedChunk],
    query_type: str = "ask",
    confidence: Optional[ConfidenceScore] = None,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ Markdown —Ñ–æ—Ä–º–∞—Ç–µ.

    Args:
        query: –í–æ–ø—Ä–æ—Å –∏–ª–∏ —Ç–µ–º–∞
        response_content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
        chunks: –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        query_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ ('ask' –∏–ª–∏ 'review')
        confidence: –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    RESULTS_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    filepath = RESULTS_DIR / f"{query_type}_{timestamp}_{safe_query}.md"

    # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = {}
    for chunk in chunks:
        if chunk.file_name not in sources:
            sources[chunk.file_name] = {
                "pages": set(),
                "sections": set(),
            }
        sources[chunk.file_name]["pages"].add(chunk.page)
        sources[chunk.file_name]["sections"].add(chunk.section)

    # –§–æ—Ä–º–∏—Ä—É–µ–º Markdown –¥–æ–∫—É–º–µ–Ω—Ç
    md_doc = f"""# {query}

**–î–∞—Ç–∞:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞:** {query_type}

"""

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ confidence, –µ—Å–ª–∏ –µ—Å—Ç—å
    if confidence:
        level_text = {
            ConfidenceLevel.HIGH: "–í—ã—Å–æ–∫–∞—è",
            ConfidenceLevel.MEDIUM: "–°—Ä–µ–¥–Ω—è—è",
            ConfidenceLevel.LOW: "–ù–∏–∑–∫–∞—è",
            ConfidenceLevel.VERY_LOW: "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
        }.get(confidence.level, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞")

        md_doc += f"""## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

- **–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞:** {query_type}
- **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {level_text} (score: {confidence.score:.2f})
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:** {confidence.num_sources}
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤:** {confidence.num_chunks}
- **–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è:** {confidence.avg_distance:.3f}

"""

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    md_doc += f"""## –û—Ç–≤–µ—Ç

{response_content}

"""

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if sources:
        md_doc += """## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

"""

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for source_name, info in sorted(sources.items()):
            pages = sorted(info["pages"])
            sections = sorted(info["sections"])
            pages_str = ", ".join(map(str, pages))
            sections_str = ", ".join(sections) if sections else None

            md_doc += f"- **{source_name}**\n"
            md_doc += f"  - –°—Ç—Ä–∞–Ω–∏—Ü—ã: {pages_str}\n"
            if sections_str:
                md_doc += f"  - –°–µ–∫—Ü–∏–∏: {sections_str}\n"
            md_doc += "\n"

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_doc)

    logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Markdown: {filepath}")


def _format_confidence_for_prompt(confidence: ConfidenceScore, query_type: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç confidence –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
    level_text = {
        ConfidenceLevel.HIGH: "–í–´–°–û–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –º–æ–∂–Ω–æ –¥–∞–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
        ConfidenceLevel.MEDIUM: "–°–†–ï–î–ù–Ø–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏",
        ConfidenceLevel.LOW: "–ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
        ConfidenceLevel.VERY_LOW: "–û–ß–ï–ù–¨ –ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω, —è–≤–Ω–æ —Å–æ–æ–±—â–∏ –æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
    }

    info = f"""–£—Ä–æ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {level_text.get(confidence.level, '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}
–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources}
–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {confidence.avg_distance:.3f}"""

    if confidence.warnings:
        info += f"\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {'; '.join(confidence.warnings)}"
    return info


def _print_confidence(confidence: ConfidenceScore) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ confidence."""
    level_styles = {
        ConfidenceLevel.HIGH: ("green", "‚úì"),
        ConfidenceLevel.MEDIUM: ("yellow", "‚óê"),
        ConfidenceLevel.LOW: ("orange1", "‚óî"),
        ConfidenceLevel.VERY_LOW: ("red", "‚úó"),
    }

    color, icon = level_styles.get(confidence.level, ("white", "?"))
    level_text = {
        ConfidenceLevel.HIGH: "–í—ã—Å–æ–∫–∞—è",
        ConfidenceLevel.MEDIUM: "–°—Ä–µ–¥–Ω—è—è",
        ConfidenceLevel.LOW: "–ù–∏–∑–∫–∞—è",
        ConfidenceLevel.VERY_LOW: "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
    }.get(confidence.level, "?")

    info_lines = [
        f"[{color}]{icon} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {level_text}[/{color}] (score: {confidence.score:.2f})",
        f"   –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources} | –ß–∞–Ω–∫–æ–≤: {confidence.num_chunks} | –°—Ä. –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {confidence.avg_distance:.3f}",
    ]

    if confidence.coverage_by_section:
        sections_str = ", ".join(
            f"{s}: {c}" for s, c in sorted(confidence.coverage_by_section.items())
        )
        info_lines.append(f"   –°–µ–∫—Ü–∏–∏: {sections_str}")

    if confidence.warnings:
        info_lines.append(f"[yellow]   ‚ö† {'; '.join(confidence.warnings)}[/yellow]")

    console.print(
        Panel(
            "\n".join(info_lines),
            title="[bold]–û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞[/bold]",
            border_style=color,
            expand=False,
        )
    )
    console.print()


def _print_sources_table(chunks: list[RetrievedChunk]) -> None:
    """–í—ã–≤–æ–¥–∏—Ç —Ç–∞–±–ª–∏—Ü—É –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    if not chunks:
        return

    console.print(Rule("[bold]–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏[/bold]"))

    seen = {}
    for chunk in chunks:
        key = chunk.file_name
        if key not in seen:
            seen[key] = {"pages": set(), "sections": set(), "distances": []}
        seen[key]["pages"].add(chunk.page)
        seen[key]["sections"].add(chunk.section)
        seen[key]["distances"].append(chunk.distance)

    table = Table(show_header=True, header_style="bold")
    table.add_column("–ò—Å—Ç–æ—á–Ω–∏–∫", width=45)
    table.add_column("–°—Ç—Ä–∞–Ω–∏—Ü—ã", width=12)
    table.add_column("–°–µ–∫—Ü–∏–∏", width=18)
    table.add_column("–†–µ–ª–µ–≤.", width=8)

    for fname, info in sorted(seen.items()):
        pages = ", ".join(map(str, sorted(info["pages"])))
        sections = ", ".join(sorted(info["sections"]))
        avg_dist = sum(info["distances"]) / len(info["distances"])
        table.add_row(fname, pages, sections, f"{1-avg_dist:.2f}")

    console.print(table)


def answer_question(
    question: str,
    db_name: str,
    n_results: int = 5,
    expand_context: bool = True,
) -> None:
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –Ω–∞—É—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

    Args:
        question: –í–æ–ø—Ä–æ—Å –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        expand_context: –†–∞—Å—à–∏—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
    """
    llm = create_llm()

    initial_chunks, query_type, confidence = retrieve_with_reranking(
        question, db_name, n_results, fetch_multiplier=2
    )

    if not initial_chunks:
        console.print("[red]–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.[/red]")
        return

    # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥—è–º–∏
    expanded_chunks = []
    if expand_context:
        seen_ids = {f"{c.file_hash}_{c.chunk_id}" for c in initial_chunks}
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–ø-—á–∞–Ω–∫–æ–≤
        top_n_for_expansion = min(EXPAND_TOP_N, len(initial_chunks))
        for chunk in initial_chunks[:top_n_for_expansion]:
            # –ü–µ—Ä–µ–¥–∞–µ–º query –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ distance –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π window
            neighbors = get_neighbor_chunks(
                chunk, db_name, window=EXPAND_WINDOW, query=question
            )
            for n in neighbors:
                key = f"{n.file_hash}_{n.chunk_id}"
                if key not in seen_ids:
                    expanded_chunks.append(n)
                    seen_ids.add(key)
        chunks = expanded_chunks + initial_chunks[top_n_for_expansion:]
    else:
        chunks = initial_chunks

    _save_chunks_to_json(chunks, question, expanded_chunks)

    context = format_context_with_citations(chunks[:10])
    confidence_info = _format_confidence_for_prompt(confidence, query_type)

    response = (QA_PROMPT | llm).invoke(
        {
            "question": question,
            "context": context,
            "confidence_info": confidence_info,
        }
    )

    _save_response_to_json(
        {"question": question, "context": context, "confidence": confidence.to_dict()},
        response,
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Markdown
    _save_to_markdown(
        query=question,
        response_content=response.content,
        chunks=chunks,
        query_type="ask",
        confidence=confidence,
    )

    # –í—ã–≤–æ–¥
    console.print(Rule("[bold blue]–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å[/bold blue]"))
    _print_confidence(confidence)
    console.print(
        Panel(
            Markdown(response.content),
            title="[bold green]–û—Ç–≤–µ—Ç[/bold green]",
            border_style="green",
            expand=True,
        )
    )
    _print_sources_table(chunks)


def review_topic(
    topic: str,
    db_name: str,
    n_results: int = 15,
    sections: Optional[list[str]] = None,
) -> None:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ.

    Args:
        topic: –¢–µ–º–∞ –¥–ª—è –æ–±–∑–æ—Ä–∞
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        sections: –°–ø–∏—Å–æ–∫ —Å–µ–∫—Ü–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    llm = create_llm()
    query_type = detect_query_type(topic)

    if sections:
        all_chunks = []
        for section in sections:
            chunks, _, _ = retrieve_with_reranking(
                topic, db_name, n_results // len(sections), section_filter=section
            )
            all_chunks.extend(chunks)
    else:
        all_chunks, query_type, confidence = retrieve_with_reranking(
            topic, db_name, n_results, fetch_multiplier=2
        )

    if not all_chunks:
        console.print("[red]–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–∑–æ—Ä–∞.[/red]")
        return

    confidence = calculate_confidence(all_chunks, query_type)
    _save_chunks_to_json(all_chunks, topic)

    context = format_context_with_citations(all_chunks)

    sources_detail = []
    seen = set()
    for chunk in all_chunks:
        if chunk.file_name not in seen:
            sources_detail.append(f"‚Ä¢ {chunk.file_name}")
            seen.add(chunk.file_name)

    confidence_info = _format_confidence_for_prompt(confidence, query_type)

    response = (REVIEW_PROMPT | llm).invoke(
        {
            "topic": topic,
            "context": context[:8000],
            "sources": "\n".join(sources_detail),
            "confidence_info": confidence_info,
        }
    )

    _save_response_to_json(
        {"topic": topic, "context": context[:8000], "confidence": confidence.to_dict()},
        response,
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Markdown
    _save_to_markdown(
        query=topic,
        response_content=response.content,
        chunks=all_chunks,
        query_type="review",
        confidence=confidence,
    )

    # –í—ã–≤–æ–¥
    console.print(Rule("[bold blue]–û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã[/bold blue]"))
    _print_confidence(confidence)
    console.print(
        Panel(
            Markdown(response.content),
            title="[bold green]–û–±–∑–æ—Ä[/bold green]",
            border_style="green",
            expand=True,
        )
    )
    _print_sources_table(all_chunks)


def search_chunks(
    query: str,
    db_name: str,
    n_results: int = 10,
    section: Optional[str] = None,
) -> None:
    """–ü–æ–∏—Å–∫ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (–±–µ–∑ LLM).

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        db_name: –ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        section: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    chunks = retrieve_chunks(query, db_name, n_results, section)

    if not chunks:
        console.print("[red]–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.[/red]")
        return

    console.print(Rule(f"[bold blue]–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: {query}[/bold blue]"))

    table = Table(show_header=True, header_style="bold")
    table.add_column("#", width=3)
    table.add_column("–ò—Å—Ç–æ—á–Ω–∏–∫", width=30)
    table.add_column("–°—Ç—Ä.", width=5)
    table.add_column("–°–µ–∫—Ü–∏—è", width=12)
    table.add_column("Dist", width=6)

    for i, chunk in enumerate(chunks, 1):
        table.add_row(
            str(i),
            chunk.file_name[:28],
            str(chunk.page),
            chunk.section,
            f"{chunk.distance:.3f}",
        )

    console.print(table)
    console.print()

    for i, chunk in enumerate(chunks, 1):
        console.print(
            Panel(
                chunk.text[:500] + ("..." if len(chunk.text) > 500 else ""),
                title=f"[{i}] {chunk.citation()}",
                subtitle=f"–°–µ–∫—Ü–∏—è: {chunk.section}",
            )
        )
