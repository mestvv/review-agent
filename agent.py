"""
–ê–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG –±–∞–∑–æ–π –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –û–±–∑–æ—Ä—ã –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
- Section-aware re-ranking –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
- Confidence score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
- Cross-chunk synthesis –∫–æ–Ω—Ç—Ä–æ–ª—å (citation validation)
- –≠–∫—Å–ø–æ—Ä—Ç –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple
from dataclasses import dataclass, asdict, field
from enum import Enum

import chromadb
from sentence_transformers import SentenceTransformer
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.rule import Rule
from rich.table import Table
from rich.text import Text
from rich.progress import Progress, SpinnerColumn, TextColumn


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

console = Console()

# ============ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ============

from config import (
    CHROMA_DB_PATH,
    COLLECTION_NAME,
    CHUNKS_LOG_DIR,
    RESPONSES_LOG_DIR,
    LATEX_OUTPUT_DIR,
    LLM_MODEL,
    LLM_BASE_URL,
    LLM_API_KEY,
    LLM_TEMPERATURE,
    SENTENCE_TRANSFORMER_MODEL,
    SECTION_WEIGHTS,
    QUERY_TYPE_KEYWORDS,
    CONFIDENCE_THRESHOLDS,
    MIN_CHUNKS_FOR_CONFIDENT_ANSWER,
    MAX_AVG_DISTANCE_FOR_CONFIDENT,
    RERANKER_MODEL,
    RERANKER_TOP_K,
    USE_RERANKER,
)

client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = client.get_or_create_collection(COLLECTION_NAME)

embedding_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)

llm = ChatOpenAI(
    model=LLM_MODEL,
    base_url=LLM_BASE_URL,
    api_key=LLM_API_KEY,
    temperature=LLM_TEMPERATURE,
    extra_body={"thinking": {"type": "enabled"}},
)

# ============ Reranker (lazy loading) ============

_reranker_model = None


def get_reranker():
    """Lazy loading reranker –º–æ–¥–µ–ª–∏."""
    global _reranker_model

    if not USE_RERANKER or not RERANKER_MODEL:
        return None

    if _reranker_model is None:
        try:
            from sentence_transformers import CrossEncoder

            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ reranker –º–æ–¥–µ–ª–∏: {RERANKER_MODEL}")
            _reranker_model = CrossEncoder(
                RERANKER_MODEL,
                trust_remote_code=True,
                # –î–ª—è CPU –∏–ª–∏ GPU –±–µ–∑ flash attention:
                # model_kwargs={"use_flash_attn": False}
            )
            logger.info("‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker: {e}")
            logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É —Å –≤–µ—Å–∞–º–∏ —Å–µ–∫—Ü–∏–π")
            return None

    return _reranker_model


# ============ –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ============


class ConfidenceLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ."""

    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    VERY_LOW = "very_low"


@dataclass
class RetrievedChunk:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —á–∞–Ω–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""

    text: str
    file_name: str
    file_hash: str
    chunk_id: int
    page: int
    section: str
    distance: float
    reranked_score: float = 0.0  # Score –ø–æ—Å–ª–µ re-ranking

    def citation(self) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Ü–∏—Ç–∞—Ç—É –¥–ª—è —Å—Å—ã–ª–∫–∏."""
        return f"[{self.file_name}, —Å—Ç—Ä. {self.page}]"

    def full_citation(self) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é —Ü–∏—Ç–∞—Ç—É."""
        return f"{self.file_name} (—Å—Ç—Ä. {self.page}, {self.section})"

    def latex_citation(self, cite_key: str) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç LaTeX —Ü–∏—Ç–∞—Ç—É."""
        return f"\\cite{{{cite_key}}}"


@dataclass
class ConfidenceScore:
    """–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞."""

    level: ConfidenceLevel
    score: float  # 0.0 - 1.0
    avg_distance: float
    min_distance: float
    max_distance: float
    num_chunks: int
    num_sources: int
    coverage_by_section: dict = field(default_factory=dict)
    warnings: list = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "level": self.level.value,
            "score": round(self.score, 3),
            "avg_distance": round(self.avg_distance, 3),
            "min_distance": round(self.min_distance, 3),
            "max_distance": round(self.max_distance, 3),
            "num_chunks": self.num_chunks,
            "num_sources": self.num_sources,
            "coverage_by_section": self.coverage_by_section,
            "warnings": self.warnings,
        }


@dataclass
class CitationValidation:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""

    is_valid: bool
    claims_without_citation: list = field(default_factory=list)
    mixed_sources_in_claim: list = field(default_factory=list)
    citation_mapping: dict = field(default_factory=dict)  # claim -> [citations]

    def to_dict(self) -> dict:
        return {
            "is_valid": self.is_valid,
            "claims_without_citation": self.claims_without_citation,
            "mixed_sources_in_claim": self.mixed_sources_in_claim,
            "citation_mapping": self.citation_mapping,
        }


# ============ Retrieval —Ñ—É–Ω–∫—Ü–∏–∏ ============


def save_chunks_to_json(
    chunks: list[RetrievedChunk],
    query: str,
    expanded_chunks: Optional[list[RetrievedChunk]] = None,
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö –≤ JSON —Ñ–∞–π–ª.

    Args:
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        expanded_chunks: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    CHUNKS_LOG_DIR.mkdir(exist_ok=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # –û—á–∏—â–∞–µ–º query –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (—É–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    filename = f"chunks_{timestamp}_{safe_query}.json"
    filepath = CHUNKS_LOG_DIR / filename

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —á–∞–Ω–∫–∏ –±—ã–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã
    expanded_ids = set()
    if expanded_chunks:
        expanded_ids = {f"{c.file_hash}_{c.chunk_id}" for c in expanded_chunks}

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    chunks_data = []
    for chunk in chunks:
        chunk_key = f"{chunk.file_hash}_{chunk.chunk_id}"
        chunks_data.append(
            {
                "chunk_id": chunk.chunk_id,
                "file_name": chunk.file_name,
                "file_hash": chunk.file_hash,
                "page": chunk.page,
                "section": chunk.section,
                "distance": chunk.distance,
                "text": chunk.text,
                "is_expanded": chunk_key in expanded_ids,
            }
        )

    data = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "total_chunks": len(chunks),
        "expanded_chunks_count": len(expanded_chunks) if expanded_chunks else 0,
        "sources": sorted(set(c.file_name for c in chunks)),
        "chunks": chunks_data,
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    logger.info(f"üíæ –ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def serialize_response(response) -> dict:
    """
    –°–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ LLM (AIMessage) –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON.

    Args:
        response: –û–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞
    """
    # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å –º–µ—Ç–æ–¥–æ–º dict() (LangChain —Å–æ–æ–±—â–µ–Ω–∏—è)
    if hasattr(response, "dict"):
        return response.dict()
    # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å –º–µ—Ç–æ–¥–æ–º model_dump() (Pydantic v2)
    elif hasattr(response, "model_dump"):
        return response.model_dump()
    # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å __dict__
    elif hasattr(response, "__dict__"):
        result = {}
        for key, value in response.__dict__.items():
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
            if hasattr(value, "dict"):
                result[key] = value.dict()
            elif hasattr(value, "model_dump"):
                result[key] = value.model_dump()
            elif hasattr(value, "__dict__"):
                result[key] = serialize_response(value)
            else:
                result[key] = value
        return result
    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ª–æ–≤–∞—Ä—å –∏–ª–∏ –ø—Ä–∏–º–∏—Ç–∏–≤
    else:
        return response


def save_response_to_json(query, response) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç LLM –≤ JSON —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Å–ª—É–∂–µ–±–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.

    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å)
        response: –ü–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM (AIMessage)
    """
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    RESPONSES_LOG_DIR.mkdir(exist_ok=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    if isinstance(query, dict):
        # –ë–µ—Ä—ë–º "question" –∏–ª–∏ "topic" –∏–∑ —Å–ª–æ–≤–∞—Ä—è
        query_str = query.get("question") or query.get("topic") or str(query)
    else:
        query_str = query

    # –û—á–∏—â–∞–µ–º query –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (—É–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
    safe_query = re.sub(r"[^\w\s-]", "", str(query_str)[:50]).strip().replace(" ", "_")
    filename = f"response_{timestamp}_{safe_query}.json"
    filepath = RESPONSES_LOG_DIR / filename

    # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º response –≤ —Å–ª–æ–≤–∞—Ä—å
    response_dict = serialize_response(response)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    data = {
        "query": query,
        "response": response_dict,
        "timestamp": datetime.now().isoformat(),
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)

    logger.info(f"üíæ –û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {filepath}")


def retrieve_chunks(
    query: str,
    n_results: int = 5,
    section_filter: Optional[str] = None,
) -> list[RetrievedChunk]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –±–∞–∑—ã.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        section_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏ (introduction, methods, results, discussion, conclusion)

    Returns:
        –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    logger.info(f"üîç –ü–æ–∏—Å–∫: '{query[:60]}...'")

    embedding = embedding_model.encode([query]).tolist()

    where_filter = None
    if section_filter:
        where_filter = {"section": section_filter}

    results = collection.query(
        query_embeddings=embedding,
        n_results=n_results,
        where=where_filter,
        include=["documents", "metadatas", "distances"],
    )

    chunks = []
    for doc, meta, dist in zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0],
    ):
        chunks.append(
            RetrievedChunk(
                text=doc,
                file_name=meta.get("file_name", "unknown"),
                file_hash=meta.get("file_hash", ""),
                chunk_id=meta.get("chunk_id", 0),
                page=meta.get("page", 0),
                section=meta.get("section", "unknown"),
                distance=dist,
            )
        )

    sources = sorted(set(c.file_name for c in chunks))
    logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    return chunks


def get_neighbor_chunks(chunk: RetrievedChunk, window: int = 1) -> list[RetrievedChunk]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

    Args:
        chunk: –ë–∞–∑–æ–≤—ã–π —á–∞–Ω–∫
        window: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ (–≤–∫–ª—é—á–∞—è –∏—Å—Ö–æ–¥–Ω—ã–π)
    """
    neighbor_ids = []
    for offset in range(-window, window + 1):
        neighbor_id = f"{chunk.file_hash}_{chunk.chunk_id + offset}"
        neighbor_ids.append(neighbor_id)

    results = collection.get(
        ids=neighbor_ids,
        include=["documents", "metadatas"],
    )

    neighbors = []
    for doc, meta in zip(results["documents"], results["metadatas"]):
        if doc and meta:
            neighbors.append(
                RetrievedChunk(
                    text=doc,
                    file_name=meta.get("file_name", "unknown"),
                    file_hash=meta.get("file_hash", ""),
                    chunk_id=meta.get("chunk_id", 0),
                    page=meta.get("page", 0),
                    section=meta.get("section", "unknown"),
                    distance=0.0,
                )
            )

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_id
    return sorted(neighbors, key=lambda c: c.chunk_id)


def format_context_with_citations(chunks: list[RetrievedChunk]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    formatted = []
    for i, chunk in enumerate(chunks, 1):
        formatted.append(
            f"[{i}] {chunk.citation()}\n"
            f"–°–µ–∫—Ü–∏—è: {chunk.section}\n"
            f"---\n{chunk.text}\n"
        )
    return "\n".join(formatted)


# ============ Section-aware Re-ranking ============


def detect_query_type(query: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.

    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞

    Returns:
        –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: 'results', 'methods', 'definitions', 'overview' –∏–ª–∏ 'default'
    """
    query_lower = query.lower()

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
    type_scores = {}
    for query_type, keywords in QUERY_TYPE_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in query_lower)
        if score > 0:
            type_scores[query_type] = score

    if not type_scores:
        return "default"

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∏–ø —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    return max(type_scores, key=type_scores.get)


def rerank_chunks_with_model(
    query: str,
    chunks: list[RetrievedChunk],
    top_k: Optional[int] = None,
) -> list[RetrievedChunk]:
    """
    –†–µ-—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é CrossEncoder reranker –º–æ–¥–µ–ª–∏.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (None = –≤—Å–µ)

    Returns:
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –ø–æ reranked_score
    """
    reranker = get_reranker()

    if reranker is None or not chunks:
        return chunks

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è reranker
    documents = [chunk.text for chunk in chunks]

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ rank() –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = reranker.rank(
            query,
            documents,
            return_documents=False,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω–¥–µ–∫—Å—ã –∏ scores
            top_k=top_k or len(chunks),
        )

        # results - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å 'corpus_id' –∏ 'score'
        reranked_chunks = []
        for result in results:
            idx = result["corpus_id"]
            score = result["score"]
            chunk = chunks[idx]
            chunk.reranked_score = score
            reranked_chunks.append(chunk)

        logger.info(
            f"üéØ Reranker: —Ç–æ–ø score={reranked_chunks[0].reranked_score:.3f}, "
            f"–º–∏–Ω score={reranked_chunks[-1].reranked_score:.3f}"
        )
        return reranked_chunks

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ reranker: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        return chunks


def rerank_chunks_heuristic(
    chunks: list[RetrievedChunk],
    query_type: str,
) -> list[RetrievedChunk]:
    """
    –†–µ-—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π —Å –≤–µ—Å–∞–º–∏ —Å–µ–∫—Ü–∏–π (fallback).

    Args:
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –±–∞–∑–æ–≤—ã–º distance
        query_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ ('results', 'methods', etc.)

    Returns:
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –ø–æ reranked_score
    """
    weights = SECTION_WEIGHTS.get(query_type, SECTION_WEIGHTS["default"])

    for chunk in chunks:
        section_weight = weights.get(chunk.section, 1.0)
        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º distance (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ) –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å —Å–µ–∫—Ü–∏–∏
        chunk.reranked_score = (1.0 - chunk.distance) * section_weight

    return sorted(chunks, key=lambda c: c.reranked_score, reverse=True)


def rerank_chunks(
    query: str,
    chunks: list[RetrievedChunk],
    query_type: str,
    top_k: Optional[int] = None,
) -> list[RetrievedChunk]:
    """
    –†–µ-—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ ‚Äî —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ—Ç ML –º–æ–¥–µ–ª—å, –∑–∞—Ç–µ–º fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        query_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è fallback —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    if not chunks:
        return chunks

    reranker = get_reranker()

    if reranker is not None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ML reranker
        reranked = rerank_chunks_with_model(query, chunks, top_k)
        if reranked and reranked[0].reranked_score > 0:
            return reranked[:top_k] if top_k else reranked

    # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É
    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É —Å –≤–µ—Å–∞–º–∏ —Å–µ–∫—Ü–∏–π")
    reranked = rerank_chunks_heuristic(chunks, query_type)
    return reranked[:top_k] if top_k else reranked


def retrieve_with_reranking(
    query: str,
    n_results: int = 5,
    section_filter: Optional[str] = None,
    fetch_multiplier: int = 3,
) -> tuple[list[RetrievedChunk], str, ConfidenceScore]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∞–Ω–∫–∏ —Å re-ranking (ML –º–æ–¥–µ–ª—å –∏–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞).

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ re-ranking
        section_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏
        fetch_multiplier: –ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ fetch

    Returns:
        (reranked_chunks, query_type, confidence_score)
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è fallback —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –∏ confidence)
    query_type = detect_query_type(query)
    logger.info(f"üéØ –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ re-ranking
    # –î–ª—è ML reranker –≤–∞–∂–Ω–æ –∏–º–µ—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    fetch_count = max(n_results * fetch_multiplier, RERANKER_TOP_K)
    initial_chunks = retrieve_chunks(query, fetch_count, section_filter)

    if not initial_chunks:
        return [], query_type, calculate_confidence([], query_type)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º re-ranking
    top_chunks = rerank_chunks(
        query=query,
        chunks=initial_chunks,
        query_type=query_type,
        top_k=n_results,
    )

    # –í—ã—á–∏—Å–ª—è–µ–º confidence score
    confidence = calculate_confidence(top_chunks, query_type)

    # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ ranking
    changes = sum(
        1
        for i, c in enumerate(top_chunks)
        if i < len(initial_chunks) and c != initial_chunks[i]
    )
    logger.info(f"üìä Re-ranking: –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Ç–æ–ø-{n_results}: {changes}")

    return top_chunks, query_type, confidence


# ============ Confidence Score ============


def calculate_confidence(
    chunks: list[RetrievedChunk],
    query_type: str,
) -> ConfidenceScore:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ retrieval metrics.

    Args:
        chunks: –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        query_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞

    Returns:
        ConfidenceScore —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    """
    warnings = []

    if not chunks:
        return ConfidenceScore(
            level=ConfidenceLevel.VERY_LOW,
            score=0.0,
            avg_distance=1.0,
            min_distance=1.0,
            max_distance=1.0,
            num_chunks=0,
            num_sources=0,
            warnings=["–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"],
        )

    distances = [c.distance for c in chunks]
    avg_distance = sum(distances) / len(distances)
    min_distance = min(distances)
    max_distance = max(distances)

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    sources = set(c.file_name for c in chunks)
    num_sources = len(sources)

    # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Å–µ–∫—Ü–∏—è–º
    section_coverage = {}
    for chunk in chunks:
        section_coverage[chunk.section] = section_coverage.get(chunk.section, 0) + 1

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    if avg_distance < CONFIDENCE_THRESHOLDS["high"]:
        level = ConfidenceLevel.HIGH
        base_score = 0.9
    elif avg_distance < CONFIDENCE_THRESHOLDS["medium"]:
        level = ConfidenceLevel.MEDIUM
        base_score = 0.7
    elif avg_distance < CONFIDENCE_THRESHOLDS["low"]:
        level = ConfidenceLevel.LOW
        base_score = 0.5
    else:
        level = ConfidenceLevel.VERY_LOW
        base_score = 0.3

    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ score
    score = base_score

    # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
    if len(chunks) >= MIN_CHUNKS_FOR_CONFIDENT_ANSWER:
        score += 0.05
    else:
        warnings.append(
            f"–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(chunks)} —á–∞–Ω–∫–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚â•{MIN_CHUNKS_FOR_CONFIDENT_ANSWER})"
        )
        score -= 0.1

    # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    if num_sources >= 2:
        score += 0.05
    else:
        warnings.append("–û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–¥–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ")

    # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ã—Å–æ–∫–∏–π —Ä–∞–∑–±—Ä–æ—Å distances
    distance_spread = max_distance - min_distance
    if distance_spread > 0.4:
        warnings.append(f"–í—ã—Å–æ–∫–∏–π —Ä–∞–∑–±—Ä–æ—Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {distance_spread:.2f}")
        score -= 0.05

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –≤–∞–∂–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –¥–ª—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
    important_sections = {
        "results": ["results", "conclusion", "discussion"],
        "methods": ["methods"],
        "definitions": ["introduction", "methods"],
        "overview": ["abstract", "introduction", "conclusion"],
    }

    if query_type in important_sections:
        covered = [s for s in important_sections[query_type] if s in section_coverage]
        if not covered:
            warnings.append(
                f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–µ–∫—Ü–∏–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Ç–∏–ø–∞ '{query_type}'"
            )
            score -= 0.1

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º score –≤ [0, 1]
    score = max(0.0, min(1.0, score))

    return ConfidenceScore(
        level=level,
        score=score,
        avg_distance=avg_distance,
        min_distance=min_distance,
        max_distance=max_distance,
        num_chunks=len(chunks),
        num_sources=num_sources,
        coverage_by_section=section_coverage,
        warnings=warnings,
    )


# ============ Cross-chunk Synthesis Control ============


def validate_citations_in_response(
    response_text: str,
    available_chunks: list[RetrievedChunk],
) -> CitationValidation:
    """
    –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ LLM.

    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –ö–∞–∂–¥–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–º–µ–µ—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É —Ü–∏—Ç–∞—Ç—É
    - –ù–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Å–º–µ—à–∏–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

    Args:
        response_text: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ LLM
        available_chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã LLM

    Returns:
        CitationValidation —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    claims_without_citation = []
    mixed_sources_in_claim = []
    citation_mapping = {}

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–§–∞–π–ª, —Å—Ç—Ä. X] –∏–ª–∏ [N]
    citation_pattern = r"\[([^\]]+(?:—Å—Ç—Ä\.|—Å—Ç—Ä|p\.|pp\.)[^\]]*)\]|\[(\d+)\]"

    # –†–∞–∑–±–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è/—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–∫—É, –Ω–æ –∏–∑–±–µ–≥–∞–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è—Ö
    sentences = re.split(r"(?<=[.!?])\s+(?=[–ê-–ØA-Z])", response_text)

    # –°–æ–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    available_sources = {c.file_name for c in available_chunks}
    source_to_chunks = {}
    for chunk in available_chunks:
        if chunk.file_name not in source_to_chunks:
            source_to_chunks[chunk.file_name] = []
        source_to_chunks[chunk.file_name].append(chunk)

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence or len(sentence) < 20:
            continue

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        skip_phrases = [
            "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ",
            "–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
            "–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç",
            "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
            "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
        ]
        if any(phrase in sentence.lower() for phrase in skip_phrases):
            continue

        # –ò—â–µ–º —Ü–∏—Ç–∞—Ç—ã –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
        citations_in_sentence = re.findall(citation_pattern, sentence)

        # –ï—Å–ª–∏ —ç—Ç–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å —Ñ–∞–∫—Ç–∞–º–∏ (—Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–∞ –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
        is_factual = bool(re.search(r"\d+", sentence)) or any(
            kw in sentence.lower()
            for kw in [
                "—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç",
                "—Ä–∞–≤–µ–Ω",
                "–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç",
                "–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç",
                "–Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è",
            ]
        )

        if is_factual and not citations_in_sentence:
            claims_without_citation.append(
                sentence[:100] + "..." if len(sentence) > 100 else sentence
            )

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ü–∏—Ç–∞—Ç–∞—Ö
        sources_in_sentence = set()
        for cite_match in citations_in_sentence:
            cite_text = cite_match[0] or cite_match[1]
            for source in available_sources:
                if source in cite_text or any(
                    part in cite_text for part in source.replace(".pdf", "").split("_")
                ):
                    sources_in_sentence.add(source)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –æ–¥–Ω–æ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏
        if len(sources_in_sentence) > 1 and is_factual:
            # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –Ω–æ –æ—Ç–º–µ—á–∞–µ–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            mixed_sources_in_claim.append(
                {
                    "claim": (
                        sentence[:100] + "..." if len(sentence) > 100 else sentence
                    ),
                    "sources": list(sources_in_sentence),
                }
            )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º mapping
        if citations_in_sentence:
            claim_key = sentence[:50]
            citation_mapping[claim_key] = [
                cite_match[0] or cite_match[1] for cite_match in citations_in_sentence
            ]

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
    is_valid = len(claims_without_citation) == 0

    return CitationValidation(
        is_valid=is_valid,
        claims_without_citation=claims_without_citation,
        mixed_sources_in_claim=mixed_sources_in_claim,
        citation_mapping=citation_mapping,
    )


# ============ LaTeX Export ============


def generate_latex_document(
    title: str,
    content: str,
    chunks: list[RetrievedChunk],
    confidence: Optional[ConfidenceScore] = None,
    query: str = "",
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LaTeX –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –∞–≥–µ–Ω—Ç–∞.

    Args:
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        content: –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        chunks: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
        confidence: –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å

    Returns:
        –°—Ç—Ä–æ–∫–∞ —Å LaTeX –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
    """
    # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
    sources = {}
    for chunk in chunks:
        if chunk.file_name not in sources:
            sources[chunk.file_name] = {
                "pages": set(),
                "sections": set(),
                "cite_key": f"source{len(sources) + 1}",
            }
        sources[chunk.file_name]["pages"].add(chunk.page)
        sources[chunk.file_name]["sections"].add(chunk.section)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º markdown-–ø–æ–¥–æ–±–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ LaTeX
    latex_content = convert_to_latex(content, sources)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    document = f"""\\documentclass[12pt,a4paper]{{article}}
\\usepackage[utf8]{{inputenc}}
\\usepackage[T2A]{{fontenc}}
\\usepackage[russian]{{babel}}
\\usepackage{{hyperref}}
\\usepackage{{geometry}}
\\usepackage{{natbib}}
\\usepackage{{booktabs}}
\\usepackage{{graphicx}}

\\geometry{{margin=2.5cm}}

\\title{{{escape_latex(title)}}}
\\author{{RAG Literature Agent}}
\\date{{\\today}}

\\begin{{document}}

\\maketitle

"""

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—Ä–æ—Å–µ
    if query:
        document += f"""\\section*{{–ó–∞–ø—Ä–æ—Å}}
\\textit{{{escape_latex(query)}}}

"""

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    if confidence:
        confidence_text = {
            ConfidenceLevel.HIGH: "–í—ã—Å–æ–∫–∞—è",
            ConfidenceLevel.MEDIUM: "–°—Ä–µ–¥–Ω—è—è",
            ConfidenceLevel.LOW: "–ù–∏–∑–∫–∞—è",
            ConfidenceLevel.VERY_LOW: "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
        }.get(confidence.level, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞")

        document += f"""\\section*{{–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞}}
\\begin{{itemize}}
    \\item –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: \\textbf{{{confidence_text}}} (score: {confidence.score:.2f})
    \\item –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {confidence.avg_distance:.3f}
    \\item –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources}
    \\item –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {confidence.num_chunks}
\\end{{itemize}}

"""
        if confidence.warnings:
            document += "\\textbf{–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:}\n\\begin{itemize}\n"
            for warning in confidence.warnings:
                document += f"    \\item {escape_latex(warning)}\n"
            document += "\\end{itemize}\n\n"

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    document += f"""\\section*{{–û—Ç–≤–µ—Ç}}

{latex_content}

"""

    # –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è
    document += """\\section*{–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏}

\\begin{thebibliography}{99}

"""

    for fname, info in sorted(sources.items()):
        pages = ", ".join(map(str, sorted(info["pages"])))
        sections = ", ".join(sorted(info["sections"]))
        document += f"""\\bibitem{{{info['cite_key']}}}
{escape_latex(fname)}, —Å—Ç—Ä. {pages}. –°–µ–∫—Ü–∏–∏: {sections}.

"""

    document += """\\end{thebibliography}

\\end{document}
"""

    return document


def convert_to_latex(text: str, sources: dict) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–≤–æ–∑–º–æ–∂–Ω–æ —Å markdown)
        sources: –°–ª–æ–≤–∞—Ä—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∑–∞–º–µ–Ω—ã —Ü–∏—Ç–∞—Ç

    Returns:
        –¢–µ–∫—Å—Ç –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ
    """
    result = text

    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã LaTeX (–∫—Ä–æ–º–µ —Ç–µ—Ö, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º)
    special_chars = ["%", "&", "#", "_"]
    for char in special_chars:
        result = result.replace(char, f"\\{char}")

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
    result = re.sub(r"^### (.+)$", r"\\subsubsection*{\1}", result, flags=re.MULTILINE)
    result = re.sub(r"^## (.+)$", r"\\subsection*{\1}", result, flags=re.MULTILINE)
    result = re.sub(r"^# (.+)$", r"\\section*{\1}", result, flags=re.MULTILINE)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º **bold** –≤ \textbf{}
    result = re.sub(r"\*\*(.+?)\*\*", r"\\textbf{\1}", result)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º *italic* –≤ \textit{}
    result = re.sub(r"\*(.+?)\*", r"\\textit{\1}", result)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏
    lines = result.split("\n")
    in_list = False
    new_lines = []

    for line in lines:
        if re.match(r"^\s*[-‚Ä¢]\s+", line):
            if not in_list:
                new_lines.append("\\begin{itemize}")
                in_list = True
            item_text = re.sub(r"^\s*[-‚Ä¢]\s+", "", line)
            new_lines.append(f"    \\item {item_text}")
        else:
            if in_list:
                new_lines.append("\\end{itemize}")
                in_list = False
            new_lines.append(line)

    if in_list:
        new_lines.append("\\end{itemize}")

    result = "\n".join(new_lines)

    # –ó–∞–º–µ–Ω—è–µ–º —Ü–∏—Ç–∞—Ç—ã –Ω–∞ LaTeX —Ñ–æ—Ä–º–∞—Ç
    for fname, info in sources.items():
        # –ó–∞–º–µ–Ω—è–µ–º [filename, —Å—Ç—Ä. X] –Ω–∞ \cite{key}
        pattern = re.escape(f"[{fname}")
        result = re.sub(
            pattern + r"[^\]]*\]", f'\\\\cite{{{info["cite_key"]}}}', result
        )

    return result


def escape_latex(text: str) -> str:
    """–≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã LaTeX."""
    replacements = {
        "\\": "\\textbackslash{}",
        "{": "\\{",
        "}": "\\}",
        "$": "\\$",
        "%": "\\%",
        "&": "\\&",
        "#": "\\#",
        "_": "\\_",
        "^": "\\^{}",
        "~": "\\textasciitilde{}",
    }
    for char, replacement in replacements.items():
        text = text.replace(char, replacement)
    return text


def save_latex_document(
    latex_content: str,
    query: str,
) -> Path:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç LaTeX –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ñ–∞–π–ª.

    Args:
        latex_content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ LaTeX –¥–æ–∫—É–º–µ–Ω—Ç–∞
        query: –ó–∞–ø—Ä–æ—Å –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞

    Returns:
        –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    LATEX_OUTPUT_DIR.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query = re.sub(r"[^\w\s-]", "", query[:30]).strip().replace(" ", "_")
    filename = f"response_{timestamp}_{safe_query}.tex"
    filepath = LATEX_OUTPUT_DIR / filename

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(latex_content)

    logger.info(f"üìÑ LaTeX –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filepath}")
    return filepath


# ============ –ü—Ä–æ–º–ø—Ç—ã ============

QA_PROMPT = PromptTemplate(
    input_variables=["question", "context", "confidence_info"],
    template="""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏.
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ö–û–ù–¢–ï–ö–°–¢ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π):
{context}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–û–ù–¢–ï–ö–°–¢–ï:
{confidence_info}

–í–û–ü–†–û–°:
{question}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –¶–ò–¢–ò–†–û–í–ê–ù–ò–Æ:
1. –ö–ê–ñ–î–û–ï —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ (—á–∏—Å–ª–∞, –¥–∞–Ω–Ω—ã–µ, –≤—ã–≤–æ–¥—ã) –î–û–õ–ñ–ù–û –∏–º–µ—Ç—å —Ü–∏—Ç–∞—Ç—É
2. –§–æ—Ä–º–∞—Ç —Ü–∏—Ç–∞—Ç—ã: [–§–∞–π–ª, —Å—Ç—Ä. X]
3. –ù–ï –°–ú–ï–®–ò–í–ê–ô –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è
4. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ‚Äî —É–∫–∞–∂–∏: "–ü–æ –¥–∞–Ω–Ω—ã–º [–ò—Å—Ç–æ—á–Ω–∏–∫1], ..., —Ç–æ–≥–¥–∞ –∫–∞–∫ [–ò—Å—Ç–æ—á–Ω–∏–∫2] —É–∫–∞–∑—ã–≤–∞–µ—Ç..."

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
- –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∞–±—ã–π ‚Äî –ß–ï–°–¢–ù–û —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- –ü—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏: "—Å–æ–≥–ª–∞—Å–Ω–æ –∏–º–µ—é—â–∏–º—Å—è –¥–∞–Ω–Ω—ã–º", "–≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
""",
)

QA_PROMPT_SIMPLE = PromptTemplate(
    input_variables=["question", "context"],
    template="""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏.
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ö–û–ù–¢–ï–ö–°–¢ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π):
{context}

–í–û–ü–†–û–°:
{question}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
- –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–§–∞–π–ª, —Å—Ç—Ä. X]
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —á–µ—Å—Ç–Ω–æ —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ""",
)

REVIEW_PROMPT = PromptTemplate(
    input_variables=["topic", "context", "sources", "confidence_info"],
    template="""–¢—ã –Ω–∞—É—á–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å, –≥–æ—Ç–æ–≤—è—â–∏–π –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã.

–¢–ï–ú–ê –û–ë–ó–û–†–ê:
"{topic}"

–ò–ó–í–õ–ï–ß–Å–ù–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´ –ò–ó –ù–ê–£–ß–ù–´–• –°–¢–ê–¢–ï–ô:
{context}

–î–û–°–¢–£–ü–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:
{sources}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–û–ù–¢–ï–ö–°–¢–ï:
{confidence_info}

–ó–ê–î–ê–ß–ê:
–ù–∞–ø–∏—à–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ç–µ–º–µ.

–°–¢–†–£–ö–¢–£–†–ê –û–ë–ó–û–†–ê:
1. –í–≤–µ–¥–µ–Ω–∏–µ ‚Äî –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã
2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π ‚Äî –∫–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ –∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
3. –û–±—Å—É–∂–¥–µ–Ω–∏–µ ‚Äî —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –ø—Ä–æ–±–µ–ª—ã –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö
4. –í—ã–≤–æ–¥—ã ‚Äî –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–Ω–∞–Ω–∏–π
5. –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –¶–ò–¢–ò–†–û–í–ê–ù–ò–Æ:
1. –ö–ê–ñ–î–û–ï —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –î–û–õ–ñ–ù–û –∏–º–µ—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
2. –ù–ï –æ–±—ä–µ–¥–∏–Ω—è–π –≤—ã–≤–æ–¥—ã —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –ï—Å–ª–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç –¥—Ä—É–≥ –¥—Ä—É–≥—É ‚Äî —É–∫–∞–∂–∏ —ç—Ç–æ —è–≤–Ω–æ
4. –ü—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: "–ò—Å—Ç–æ—á–Ω–∏–∫ –ê —É–∫–∞–∑—ã–≤–∞–µ—Ç X, —Ç–æ–≥–¥–∞ –∫–∞–∫ –ò—Å—Ç–æ—á–Ω–∏–∫ –ë –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç Y"

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ù–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
- –°–∏–Ω—Ç–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ—Å–∫–∞–∑
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî –æ—Ç–º–µ—á–∞–π —ç—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ""",
)


# ============ –ê–≥–µ–Ω—Ç ============


class LiteratureAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º retrieval."""

    def __init__(self, llm):
        self.llm = llm
        self.last_confidence: Optional[ConfidenceScore] = None
        self.last_citation_validation: Optional[CitationValidation] = None

    def answer_question(
        self,
        question: str,
        n_results: int = 5,
        expand_context: bool = True,
        save_latex: bool = False,
        validate_citations: bool = True,
    ) -> None:
        """
        –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –Ω–∞—É—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

        Args:
            question: –í–æ–ø—Ä–æ—Å
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            expand_context: –†–∞—Å—à–∏—Ä—è—Ç—å –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
            save_latex: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –æ—Ç–≤–µ—Ç –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ
            validate_citations: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π retrieval —Å re-ranking
        initial_chunks, query_type, confidence = retrieve_with_reranking(
            question, n_results, fetch_multiplier=2
        )

        if not initial_chunks:
            console.print("[red]–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.[/red]")
            return

        self.last_confidence = confidence

        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥—è–º–∏ –¥–ª—è –ª—É—á—à–µ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        expanded_chunks = []
        if expand_context:
            seen_ids = {f"{c.file_hash}_{c.chunk_id}" for c in initial_chunks}
            for chunk in initial_chunks[:3]:  # –†–∞—Å—à–∏—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-3
                neighbors = get_neighbor_chunks(chunk, window=1)
                for n in neighbors:
                    key = f"{n.file_hash}_{n.chunk_id}"
                    if key not in seen_ids:
                        expanded_chunks.append(n)
                        seen_ids.add(key)
            chunks = expanded_chunks + initial_chunks[3:]
        else:
            chunks = initial_chunks

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ (–≤–∫–ª—é—á–∞—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        save_chunks_to_json(chunks, question, expanded_chunks)

        context = format_context_with_citations(chunks[:10])

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ confidence –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        confidence_info = self._format_confidence_for_prompt(confidence, query_type)

        response = (QA_PROMPT | self.llm).invoke(
            {
                "question": question,
                "context": context,
                "confidence_info": confidence_info,
            }
        )

        save_response_to_json(
            query={
                "question": question,
                "context": context,
                "confidence": confidence.to_dict(),
            },
            response=response,
        )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        citation_validation = None
        if validate_citations:
            citation_validation = validate_citations_in_response(
                response.content, chunks
            )
            self.last_citation_validation = citation_validation

        self._print_answer(
            "–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å",
            response.content,
            chunks,
            confidence=confidence,
            citation_validation=citation_validation,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ LaTeX
        if save_latex:
            latex_doc = generate_latex_document(
                title=f"–û—Ç–≤–µ—Ç: {question[:50]}...",
                content=response.content,
                chunks=chunks,
                confidence=confidence,
                query=question,
            )
            save_latex_document(latex_doc, question)

    def _format_confidence_for_prompt(
        self, confidence: ConfidenceScore, query_type: str
    ) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ confidence –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
        level_text = {
            ConfidenceLevel.HIGH: "–í–´–°–û–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –º–æ–∂–Ω–æ –¥–∞–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
            ConfidenceLevel.MEDIUM: "–°–†–ï–î–ù–Ø–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏",
            ConfidenceLevel.LOW: "–ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
            ConfidenceLevel.VERY_LOW: "–û–ß–ï–ù–¨ –ù–ò–ó–ö–ê–Ø ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω, —è–≤–Ω–æ —Å–æ–æ–±—â–∏ –æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
        }

        info = f"""–£—Ä–æ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {level_text.get(confidence.level, '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}
–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources}
–°—Ä–µ–¥–Ω—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {confidence.avg_distance:.3f}"""

        if confidence.warnings:
            info += f"\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {'; '.join(confidence.warnings)}"

        return info

    def review_topic(
        self,
        topic: str,
        n_results: int = 15,
        sections: Optional[list[str]] = None,
        save_latex: bool = False,
        validate_citations: bool = True,
    ) -> None:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ.

        Args:
            topic: –¢–µ–º–∞ –æ–±–∑–æ—Ä–∞
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
            sections: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏—è–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            save_latex: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ
            validate_citations: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        all_chunks = []
        query_type = detect_query_type(topic)

        if sections:
            # –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π —Å re-ranking
            for section in sections:
                chunks, _, _ = retrieve_with_reranking(
                    topic, n_results // len(sections), section_filter=section
                )
                all_chunks.extend(chunks)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º re-ranking –¥–ª—è –æ–±–∑–æ—Ä–∞
            all_chunks, query_type, confidence = retrieve_with_reranking(
                topic, n_results, fetch_multiplier=2
            )
            self.last_confidence = confidence

        if not all_chunks:
            console.print("[red]–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–∑–æ—Ä–∞.[/red]")
            return

        # –í—ã—á–∏—Å–ª—è–µ–º confidence –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        confidence = calculate_confidence(all_chunks, query_type)
        self.last_confidence = confidence

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ JSON
        save_chunks_to_json(all_chunks, topic)

        context = format_context_with_citations(all_chunks)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –¥–µ—Ç–∞–ª—è–º–∏
        sources_detail = []
        seen = set()
        for chunk in all_chunks:
            key = chunk.file_name
            if key not in seen:
                sources_detail.append(f"‚Ä¢ {chunk.file_name}")
                seen.add(key)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ confidence
        confidence_info = self._format_confidence_for_prompt(confidence, query_type)

        response = (REVIEW_PROMPT | self.llm).invoke(
            {
                "topic": topic,
                "context": context[:8000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                "sources": "\n".join(sources_detail),
                "confidence_info": confidence_info,
            }
        )

        save_response_to_json(
            query={
                "topic": topic,
                "context": context[:8000],
                "sources": "\n".join(sources_detail),
                "confidence": confidence.to_dict(),
            },
            response=response,
        )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        citation_validation = None
        if validate_citations:
            citation_validation = validate_citations_in_response(
                response.content, all_chunks
            )
            self.last_citation_validation = citation_validation

        self._print_answer(
            "–û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã",
            response.content,
            all_chunks,
            confidence=confidence,
            citation_validation=citation_validation,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ LaTeX
        if save_latex:
            latex_doc = generate_latex_document(
                title=f"–û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã: {topic}",
                content=response.content,
                chunks=all_chunks,
                confidence=confidence,
                query=topic,
            )
            save_latex_document(latex_doc, topic)

    def search_chunks(
        self,
        query: str,
        n_results: int = 10,
        section: Optional[str] = None,
    ) -> None:
        """
        –ü–æ–∏—Å–∫ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–±–µ–∑ LLM).

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            section: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏
        """
        chunks = retrieve_chunks(query, n_results, section)

        if not chunks:
            console.print("[red]–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.[/red]")
            return

        console.print(Rule(f"[bold blue]–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: {query}[/bold blue]"))

        table = Table(show_header=True, header_style="bold")
        table.add_column("#", width=3)
        table.add_column("–ò—Å—Ç–æ—á–Ω–∏–∫", width=30)
        table.add_column("–°—Ç—Ä.", width=5)
        table.add_column("–°–µ–∫—Ü–∏—è", width=12)
        table.add_column("Dist", width=6)

        for i, chunk in enumerate(chunks, 1):
            table.add_row(
                str(i),
                chunk.file_name[:28],
                str(chunk.page),
                chunk.section,
                f"{chunk.distance:.3f}",
            )

        console.print(table)
        console.print()

        for i, chunk in enumerate(chunks, 1):
            console.print(
                Panel(
                    chunk.text[:500] + ("..." if len(chunk.text) > 500 else ""),
                    title=f"[{i}] {chunk.citation()}",
                    subtitle=f"–°–µ–∫—Ü–∏—è: {chunk.section}",
                )
            )

    # ============ –í—ã–≤–æ–¥ ============

    def _split_thinking_and_answer(self, text: str) -> Tuple[str, str]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è (thinking) –∏ –æ—Ç–≤–µ—Ç.

        Returns:
            (thinking, answer) - –∫–æ—Ä—Ç–µ–∂ –∏–∑ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –∏ –æ—Ç–≤–µ—Ç–∞
        """
        # –ò—â–µ–º —è–≤–Ω—ã–µ —Ç–µ–≥–∏ <think>
        think_patterns = [
            r"<think>(.*?)</think>",
            r"<thinking>(.*?)</thinking>",
        ]

        thinking = ""
        answer = text

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —è–≤–Ω—ã–µ —Ç–µ–≥–∏
        for pattern in think_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                thinking = match.group(1).strip()
                answer = text[: match.start()] + text[match.end() :].strip()
                return thinking, answer

        # –ï—Å–ª–∏ —Ç–µ–≥–æ–≤ –Ω–µ—Ç, –∏—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
        # –†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç–∞-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ
        thinking_phrases = [
            "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
            "–∫–∞–∂–µ—Ç—Å—è",
            "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ",
            "–≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å",
            "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å",
            "–Ω—É–∂–Ω–æ",
            "–¥–æ–ª–∂–µ–Ω",
            "–æ–∂–∏–¥–∞–µ—Ç",
            "—Ä–∞–±–æ—Ç–∞–µ—Ç —Å",
            "–∑–Ω–∞–µ—Ç",
            "—ç—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç",
            "—Ö–æ—Ç—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
            "–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º",
        ]

        lines = text.split("\n")
        thinking_lines = []
        answer_lines = []
        found_answer_start = False

        for line in lines:
            line_stripped = line.strip()

            # –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if not line_stripped:
                if found_answer_start:
                    answer_lines.append(line)
                elif thinking_lines:
                    thinking_lines.append(line)
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ
            is_thinking = any(
                phrase in line_stripped.lower() for phrase in thinking_phrases
            ) and (
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –¥–ª–∏–Ω–Ω–µ–µ –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç–∞-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                len(line_stripped) > 60
                or "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" in line_stripped.lower()
                or "–∫–æ–Ω—Ç–µ–∫—Å—Ç" in line_stripped.lower()
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞ (—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
            is_answer_start = (
                re.match(r"^[–ê-–Ø–ÅA-Z]", line_stripped)  # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
                and not any(
                    phrase in line_stripped.lower() for phrase in thinking_phrases
                )
                and (
                    "—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç" in line_stripped.lower()
                    or "—Ä–∞–≤–Ω–∞" in line_stripped.lower()
                    or "—è–≤–ª—è–µ—Ç—Å—è" in line_stripped.lower()
                    or "–Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è" in line_stripped.lower()
                    or re.search(r"\d+", line_stripped)  # –°–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã (–¥–∞–Ω–Ω—ã–µ)
                )
            )

            if is_answer_start and not found_answer_start:
                # –ù–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞–π–¥–µ–Ω–æ
                found_answer_start = True
                answer_lines.append(line)
            elif found_answer_start:
                # –ü–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç–∞ - –≤—Å—ë –∏–¥—ë—Ç –≤ –æ—Ç–≤–µ—Ç
                answer_lines.append(line)
            elif is_thinking:
                # –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ
                thinking_lines.append(line)
            elif not found_answer_start and not thinking_lines:
                # –ï—Å–ª–∏ –µ—â—ë –Ω–µ –Ω–∞—à–ª–∏ thinking –∏ –Ω–µ –Ω–∞—à–ª–∏ answer - –ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å
                # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ - —ç—Ç–æ answer
                if re.search(
                    r"\d+.*¬∞[–°C]|—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç|—Ä–∞–≤–Ω–∞", line_stripped, re.IGNORECASE
                ):
                    found_answer_start = True
                    answer_lines.append(line)
                else:
                    thinking_lines.append(line)
            else:
                # –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —Å–ª—É—á–∞–π - –¥–æ–±–∞–≤–ª—è–µ–º –≤ thinking, –µ—Å–ª–∏ –æ–Ω —É–∂–µ –Ω–∞—á–∞–ª—Å—è
                if thinking_lines:
                    thinking_lines.append(line)
                else:
                    answer_lines.append(line)

        thinking = "\n".join(thinking_lines).strip()
        answer = "\n".join(answer_lines).strip()

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∏–ª–∏ thinking —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - —Å—á–∏—Ç–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–æ–º
        if not answer or (thinking and len(thinking) > len(answer) * 1.5):
            return "", text

        return thinking, answer

    def _print_answer(
        self,
        title: str,
        text: str,
        chunks: list[RetrievedChunk],
        confidence: Optional[ConfidenceScore] = None,
        citation_validation: Optional[CitationValidation] = None,
    ) -> None:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º thinking –∏ –æ—Ç–≤–µ—Ç–∞."""
        console.print(Rule(f"[bold blue]{title}[/bold blue]"))

        # –í—ã–≤–æ–¥–∏–º Confidence Score
        if confidence:
            self._print_confidence(confidence)

        # –†–∞–∑–¥–µ–ª—è–µ–º thinking –∏ answer
        thinking, answer = self._split_thinking_and_answer(text)

        # –í—ã–≤–æ–¥–∏–º thinking –æ—Ç–¥–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å
        if thinking:
            console.print(
                Panel(
                    thinking,
                    title="[dim italic]–†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏[/dim italic]",
                    border_style="dim",
                    style="dim italic",
                    expand=False,
                )
            )
            console.print()

        # –í—ã–≤–æ–¥–∏–º —Å–∞–º –æ—Ç–≤–µ—Ç –±–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω–æ
        if answer:
            console.print(
                Panel(
                    Markdown(answer),
                    title="[bold green]–û—Ç–≤–µ—Ç[/bold green]",
                    border_style="green",
                    expand=True,
                )
            )
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –≤—ã–≤–æ–¥–∏–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ—Ç–≤–µ—Ç
            console.print(
                Panel(
                    Markdown(text),
                    title="[bold green]–û—Ç–≤–µ—Ç[/bold green]",
                    border_style="green",
                    expand=True,
                )
            )

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if citation_validation:
            self._print_citation_validation(citation_validation)

        # –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if chunks:
            console.print(Rule("[bold]–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏[/bold]"))

            seen = {}
            for chunk in chunks:
                key = chunk.file_name
                if key not in seen:
                    seen[key] = {"pages": set(), "sections": set(), "distances": []}
                seen[key]["pages"].add(chunk.page)
                seen[key]["sections"].add(chunk.section)
                seen[key]["distances"].append(chunk.distance)

            table = Table(show_header=True, header_style="bold")
            table.add_column("–ò—Å—Ç–æ—á–Ω–∏–∫", width=45)
            table.add_column("–°—Ç—Ä–∞–Ω–∏—Ü—ã", width=12)
            table.add_column("–°–µ–∫—Ü–∏–∏", width=18)
            table.add_column("–†–µ–ª–µ–≤.", width=8)

            for fname, info in sorted(seen.items()):
                pages = ", ".join(map(str, sorted(info["pages"])))
                sections = ", ".join(sorted(info["sections"]))
                avg_dist = sum(info["distances"]) / len(info["distances"])
                relevance = f"{1-avg_dist:.2f}"
                table.add_row(fname, pages, sections, relevance)

            console.print(table)

    def _print_confidence(self, confidence: ConfidenceScore) -> None:
        """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ confidence score."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –∏ —Å—Ç–∏–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
        level_styles = {
            ConfidenceLevel.HIGH: ("green", "‚úì"),
            ConfidenceLevel.MEDIUM: ("yellow", "‚óê"),
            ConfidenceLevel.LOW: ("orange1", "‚óî"),
            ConfidenceLevel.VERY_LOW: ("red", "‚úó"),
        }

        color, icon = level_styles.get(confidence.level, ("white", "?"))

        level_text = {
            ConfidenceLevel.HIGH: "–í—ã—Å–æ–∫–∞—è",
            ConfidenceLevel.MEDIUM: "–°—Ä–µ–¥–Ω—è—è",
            ConfidenceLevel.LOW: "–ù–∏–∑–∫–∞—è",
            ConfidenceLevel.VERY_LOW: "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
        }.get(confidence.level, "?")

        # –°–æ–∑–¥–∞—ë–º –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –ø–∞–Ω–µ–ª—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        info_lines = [
            f"[{color}]{icon} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {level_text}[/{color}] (score: {confidence.score:.2f})",
            f"   –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {confidence.num_sources} | –ß–∞–Ω–∫–æ–≤: {confidence.num_chunks} | –°—Ä. –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {confidence.avg_distance:.3f}",
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–∫—Ü–∏–∏
        if confidence.coverage_by_section:
            sections_str = ", ".join(
                f"{s}: {c}" for s, c in sorted(confidence.coverage_by_section.items())
            )
            info_lines.append(f"   –°–µ–∫—Ü–∏–∏: {sections_str}")

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if confidence.warnings:
            info_lines.append(f"[yellow]   ‚ö† {'; '.join(confidence.warnings)}[/yellow]")

        console.print(
            Panel(
                "\n".join(info_lines),
                title="[bold]–û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞[/bold]",
                border_style=color,
                expand=False,
            )
        )
        console.print()

    def _print_citation_validation(self, validation: CitationValidation) -> None:
        """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if validation.is_valid and not validation.mixed_sources_in_claim:
            # –í—Å—ë —Ö–æ—Ä–æ—à–æ, –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∏—á–µ–≥–æ
            return

        lines = []

        if validation.claims_without_citation:
            lines.append("[yellow]‚ö† –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±–µ–∑ —Ü–∏—Ç–∞—Ç:[/yellow]")
            for claim in validation.claims_without_citation[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                lines.append(f"  ‚Ä¢ {claim}")
            if len(validation.claims_without_citation) > 3:
                lines.append(
                    f"  ... –∏ –µ—â—ë {len(validation.claims_without_citation) - 3}"
                )

        if validation.mixed_sources_in_claim:
            lines.append("[orange1]‚ö† –í–æ–∑–º–æ–∂–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:[/orange1]")
            for mixed in validation.mixed_sources_in_claim[:2]:
                lines.append(f"  ‚Ä¢ {mixed['claim']}")
                lines.append(f"    –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(mixed['sources'])}")

        if lines:
            console.print(
                Panel(
                    "\n".join(lines),
                    title="[bold yellow]–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è[/bold yellow]",
                    border_style="yellow",
                    expand=False,
                )
            )
            console.print()


# ============ CLI ============

if __name__ == "__main__":
    agent = LiteratureAgent(llm)

    # –ü—Ä–∏–º–µ—Ä: –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
    agent.answer_question(
        "–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ—Ç–µ–ø–ª–µ–Ω–∏—è –¥–ª—è –ó–µ–º–ª–∏?",
        n_results=5,
    )
    # agent.answer_question(
    #     "–ö–∞–∫ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–≥–æ–¥–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –≤–æ–∑–¥—É—Ö–∞ –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ–¥–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–æ—Ä–Ω—ã—Ö –ø–æ—Ä–æ–¥ –Ω–∞ –≥–ª—É–±–∏–Ω–µ 1 –∏ 4 –º?",
    #     n_results=5,
    # )
    # agent.review_topic("–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∑–¥–∞–Ω–∏–π –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π")
