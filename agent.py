"""
–ê–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG –±–∞–∑–æ–π –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –û–±–∑–æ—Ä—ã –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple
from dataclasses import dataclass, asdict

import chromadb
from sentence_transformers import SentenceTransformer
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.rule import Rule
from rich.table import Table
from rich.text import Text


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

console = Console()

# ============ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ============

from config import (
    CHROMA_DB_PATH,
    COLLECTION_NAME,
    CHUNKS_LOG_DIR,
    LLM_MODEL,
    LLM_BASE_URL,
    LLM_API_KEY,
    LLM_TEMPERATURE,
    SENTENCE_TRANSFORMER_MODEL,
)

client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = client.get_or_create_collection(COLLECTION_NAME)

embedding_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)

llm = ChatOpenAI(
    model=LLM_MODEL,
    base_url=LLM_BASE_URL,
    api_key=LLM_API_KEY,
    temperature=LLM_TEMPERATURE,
)


# ============ –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ============


@dataclass
class RetrievedChunk:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —á–∞–Ω–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""

    text: str
    file_name: str
    file_hash: str
    chunk_id: int
    page: int
    section: str
    distance: float

    def citation(self) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Ü–∏—Ç–∞—Ç—É –¥–ª—è —Å—Å—ã–ª–∫–∏."""
        return f"[{self.file_name}, —Å—Ç—Ä. {self.page}]"

    def full_citation(self) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é —Ü–∏—Ç–∞—Ç—É."""
        return f"{self.file_name} (—Å—Ç—Ä. {self.page}, {self.section})"


# ============ Retrieval —Ñ—É–Ω–∫—Ü–∏–∏ ============


def save_chunks_to_json(
    chunks: list[RetrievedChunk],
    query: str,
    expanded_chunks: Optional[list[RetrievedChunk]] = None,
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö –≤ JSON —Ñ–∞–π–ª.

    Args:
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        expanded_chunks: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    CHUNKS_LOG_DIR.mkdir(exist_ok=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # –û—á–∏—â–∞–µ–º query –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (—É–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
    safe_query = re.sub(r"[^\w\s-]", "", query[:50]).strip().replace(" ", "_")
    filename = f"chunks_{timestamp}_{safe_query}.json"
    filepath = CHUNKS_LOG_DIR / filename

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —á–∞–Ω–∫–∏ –±—ã–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã
    expanded_ids = set()
    if expanded_chunks:
        expanded_ids = {f"{c.file_hash}_{c.chunk_id}" for c in expanded_chunks}

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    chunks_data = []
    for chunk in chunks:
        chunk_key = f"{chunk.file_hash}_{chunk.chunk_id}"
        chunks_data.append(
            {
                "chunk_id": chunk.chunk_id,
                "file_name": chunk.file_name,
                "file_hash": chunk.file_hash,
                "page": chunk.page,
                "section": chunk.section,
                "distance": chunk.distance,
                "text": chunk.text,
                "is_expanded": chunk_key in expanded_ids,
            }
        )

    data = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "total_chunks": len(chunks),
        "expanded_chunks_count": len(expanded_chunks) if expanded_chunks else 0,
        "sources": sorted(set(c.file_name for c in chunks)),
        "chunks": chunks_data,
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    logger.info(f"üíæ –ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def retrieve_chunks(
    query: str,
    n_results: int = 5,
    section_filter: Optional[str] = None,
) -> list[RetrievedChunk]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –±–∞–∑—ã.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        section_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏ (introduction, methods, results, discussion, conclusion)

    Returns:
        –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    logger.info(f"üîç –ü–æ–∏—Å–∫: '{query[:60]}...'")

    embedding = embedding_model.encode([query]).tolist()

    where_filter = None
    if section_filter:
        where_filter = {"section": section_filter}

    results = collection.query(
        query_embeddings=embedding,
        n_results=n_results,
        where=where_filter,
        include=["documents", "metadatas", "distances"],
    )

    chunks = []
    for doc, meta, dist in zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0],
    ):
        chunks.append(
            RetrievedChunk(
                text=doc,
                file_name=meta.get("file_name", "unknown"),
                file_hash=meta.get("file_hash", ""),
                chunk_id=meta.get("chunk_id", 0),
                page=meta.get("page", 0),
                section=meta.get("section", "unknown"),
                distance=dist,
            )
        )

    sources = sorted(set(c.file_name for c in chunks))
    logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    return chunks


def get_neighbor_chunks(chunk: RetrievedChunk, window: int = 1) -> list[RetrievedChunk]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

    Args:
        chunk: –ë–∞–∑–æ–≤—ã–π —á–∞–Ω–∫
        window: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ (–≤–∫–ª—é—á–∞—è –∏—Å—Ö–æ–¥–Ω—ã–π)
    """
    neighbor_ids = []
    for offset in range(-window, window + 1):
        neighbor_id = f"{chunk.file_hash}_{chunk.chunk_id + offset}"
        neighbor_ids.append(neighbor_id)

    results = collection.get(
        ids=neighbor_ids,
        include=["documents", "metadatas"],
    )

    neighbors = []
    for doc, meta in zip(results["documents"], results["metadatas"]):
        if doc and meta:
            neighbors.append(
                RetrievedChunk(
                    text=doc,
                    file_name=meta.get("file_name", "unknown"),
                    file_hash=meta.get("file_hash", ""),
                    chunk_id=meta.get("chunk_id", 0),
                    page=meta.get("page", 0),
                    section=meta.get("section", "unknown"),
                    distance=0.0,
                )
            )

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_id
    return sorted(neighbors, key=lambda c: c.chunk_id)


def format_context_with_citations(chunks: list[RetrievedChunk]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    formatted = []
    for i, chunk in enumerate(chunks, 1):
        formatted.append(
            f"[{i}] {chunk.citation()}\n"
            f"–°–µ–∫—Ü–∏—è: {chunk.section}\n"
            f"---\n{chunk.text}\n"
        )
    return "\n".join(formatted)


# ============ –ü—Ä–æ–º–ø—Ç—ã ============

QA_PROMPT = PromptTemplate(
    input_variables=["question", "context"],
    template="""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏.
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ö–û–ù–¢–ï–ö–°–¢ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π):
{context}

–í–û–ü–†–û–°:
{question}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
- –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–§–∞–π–ª, —Å—Ç—Ä. X]
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —á–µ—Å—Ç–Ω–æ —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ""",
)

REVIEW_PROMPT = PromptTemplate(
    input_variables=["topic", "context", "sources"],
    template="""–¢—ã –Ω–∞—É—á–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å, –≥–æ—Ç–æ–≤—è—â–∏–π –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã.

–¢–ï–ú–ê –û–ë–ó–û–†–ê:
"{topic}"

–ò–ó–í–õ–ï–ß–Å–ù–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´ –ò–ó –ù–ê–£–ß–ù–´–• –°–¢–ê–¢–ï–ô:
{context}

–î–û–°–¢–£–ü–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:
{sources}

–ó–ê–î–ê–ß–ê:
–ù–∞–ø–∏—à–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ç–µ–º–µ.

–°–¢–†–£–ö–¢–£–†–ê –û–ë–ó–û–†–ê:
1. –í–≤–µ–¥–µ–Ω–∏–µ ‚Äî –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã
2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π ‚Äî –∫–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ –∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
3. –û–±—Å—É–∂–¥–µ–Ω–∏–µ ‚Äî —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –ø—Ä–æ–±–µ–ª—ã –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö
4. –í—ã–≤–æ–¥—ã ‚Äî –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–Ω–∞–Ω–∏–π
5. –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ù–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
- –°–∏–Ω—Ç–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ—Å–∫–∞–∑
- –ö–∞–∂–¥–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö""",
)


# ============ –ê–≥–µ–Ω—Ç ============


class LiteratureAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã."""

    def __init__(self, llm):
        self.llm = llm

    def answer_question(
        self,
        question: str,
        n_results: int = 5,
        expand_context: bool = True,
    ) -> None:
        """
        –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –Ω–∞—É—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

        Args:
            question: –í–æ–ø—Ä–æ—Å
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            expand_context: –†–∞—Å—à–∏—Ä—è—Ç—å –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
        """
        initial_chunks = retrieve_chunks(question, n_results)

        if not initial_chunks:
            console.print("[red]–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.[/red]")
            return

        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥—è–º–∏ –¥–ª—è –ª—É—á—à–µ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        expanded_chunks = []
        if expand_context:
            seen_ids = {f"{c.file_hash}_{c.chunk_id}" for c in initial_chunks}
            for chunk in initial_chunks[:3]:  # –†–∞—Å—à–∏—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-3
                neighbors = get_neighbor_chunks(chunk, window=1)
                for n in neighbors:
                    key = f"{n.file_hash}_{n.chunk_id}"
                    if key not in seen_ids:
                        expanded_chunks.append(n)
                        seen_ids.add(key)
            chunks = expanded_chunks + initial_chunks[3:]
        else:
            chunks = initial_chunks

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ (–≤–∫–ª—é—á–∞—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        save_chunks_to_json(chunks, question, expanded_chunks)

        context = format_context_with_citations(chunks[:10])

        response = (QA_PROMPT | self.llm).invoke(
            {"question": question, "context": context}
        )

        self._print_answer("–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å", response.content, chunks)

    def review_topic(
        self,
        topic: str,
        n_results: int = 15,
        sections: Optional[list[str]] = None,
    ) -> None:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ.

        Args:
            topic: –¢–µ–º–∞ –æ–±–∑–æ—Ä–∞
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
            sections: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏—è–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        all_chunks = []

        if sections:
            # –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
            for section in sections:
                chunks = retrieve_chunks(topic, n_results // len(sections), section)
                all_chunks.extend(chunks)
        else:
            all_chunks = retrieve_chunks(topic, n_results)

        if not all_chunks:
            console.print("[red]–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–∑–æ—Ä–∞.[/red]")
            return

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ JSON
        save_chunks_to_json(all_chunks, topic)

        context = format_context_with_citations(all_chunks)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –¥–µ—Ç–∞–ª—è–º–∏
        sources_detail = []
        seen = set()
        for chunk in all_chunks:
            key = chunk.file_name
            if key not in seen:
                sources_detail.append(f"‚Ä¢ {chunk.file_name}")
                seen.add(key)

        response = (REVIEW_PROMPT | self.llm).invoke(
            {
                "topic": topic,
                "context": context[:8000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                "sources": "\n".join(sources_detail),
            }
        )

        self._print_answer("–û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", response.content, all_chunks)

    def search_chunks(
        self,
        query: str,
        n_results: int = 10,
        section: Optional[str] = None,
    ) -> None:
        """
        –ü–æ–∏—Å–∫ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–±–µ–∑ LLM).

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            section: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∫—Ü–∏–∏
        """
        chunks = retrieve_chunks(query, n_results, section)

        if not chunks:
            console.print("[red]–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.[/red]")
            return

        console.print(Rule(f"[bold blue]–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: {query}[/bold blue]"))

        table = Table(show_header=True, header_style="bold")
        table.add_column("#", width=3)
        table.add_column("–ò—Å—Ç–æ—á–Ω–∏–∫", width=30)
        table.add_column("–°—Ç—Ä.", width=5)
        table.add_column("–°–µ–∫—Ü–∏—è", width=12)
        table.add_column("Dist", width=6)

        for i, chunk in enumerate(chunks, 1):
            table.add_row(
                str(i),
                chunk.file_name[:28],
                str(chunk.page),
                chunk.section,
                f"{chunk.distance:.3f}",
            )

        console.print(table)
        console.print()

        for i, chunk in enumerate(chunks, 1):
            console.print(
                Panel(
                    chunk.text[:500] + ("..." if len(chunk.text) > 500 else ""),
                    title=f"[{i}] {chunk.citation()}",
                    subtitle=f"–°–µ–∫—Ü–∏—è: {chunk.section}",
                )
            )

    # ============ –í—ã–≤–æ–¥ ============

    def _split_thinking_and_answer(self, text: str) -> Tuple[str, str]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è (thinking) –∏ –æ—Ç–≤–µ—Ç.

        Returns:
            (thinking, answer) - –∫–æ—Ä—Ç–µ–∂ –∏–∑ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –∏ –æ—Ç–≤–µ—Ç–∞
        """
        # –ò—â–µ–º —è–≤–Ω—ã–µ —Ç–µ–≥–∏ <think>
        think_patterns = [
            r"<think>(.*?)</think>",
            r"<thinking>(.*?)</thinking>",
        ]

        thinking = ""
        answer = text

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —è–≤–Ω—ã–µ —Ç–µ–≥–∏
        for pattern in think_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                thinking = match.group(1).strip()
                answer = text[: match.start()] + text[match.end() :].strip()
                return thinking, answer

        # –ï—Å–ª–∏ —Ç–µ–≥–æ–≤ –Ω–µ—Ç, –∏—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
        # –†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç–∞-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ
        thinking_phrases = [
            "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
            "–∫–∞–∂–µ—Ç—Å—è",
            "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ",
            "–≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å",
            "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å",
            "–Ω—É–∂–Ω–æ",
            "–¥–æ–ª–∂–µ–Ω",
            "–æ–∂–∏–¥–∞–µ—Ç",
            "—Ä–∞–±–æ—Ç–∞–µ—Ç —Å",
            "–∑–Ω–∞–µ—Ç",
            "—ç—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç",
            "—Ö–æ—Ç—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
            "–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º",
        ]

        lines = text.split("\n")
        thinking_lines = []
        answer_lines = []
        found_answer_start = False

        for line in lines:
            line_stripped = line.strip()

            # –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if not line_stripped:
                if found_answer_start:
                    answer_lines.append(line)
                elif thinking_lines:
                    thinking_lines.append(line)
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ
            is_thinking = any(
                phrase in line_stripped.lower() for phrase in thinking_phrases
            ) and (
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –¥–ª–∏–Ω–Ω–µ–µ –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç–∞-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                len(line_stripped) > 60
                or "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" in line_stripped.lower()
                or "–∫–æ–Ω—Ç–µ–∫—Å—Ç" in line_stripped.lower()
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞ (—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
            is_answer_start = (
                re.match(r"^[–ê-–Ø–ÅA-Z]", line_stripped)  # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
                and not any(
                    phrase in line_stripped.lower() for phrase in thinking_phrases
                )
                and (
                    "—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç" in line_stripped.lower()
                    or "—Ä–∞–≤–Ω–∞" in line_stripped.lower()
                    or "—è–≤–ª—è–µ—Ç—Å—è" in line_stripped.lower()
                    or "–Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è" in line_stripped.lower()
                    or re.search(r"\d+", line_stripped)  # –°–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã (–¥–∞–Ω–Ω—ã–µ)
                )
            )

            if is_answer_start and not found_answer_start:
                # –ù–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞–π–¥–µ–Ω–æ
                found_answer_start = True
                answer_lines.append(line)
            elif found_answer_start:
                # –ü–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç–∞ - –≤—Å—ë –∏–¥—ë—Ç –≤ –æ—Ç–≤–µ—Ç
                answer_lines.append(line)
            elif is_thinking:
                # –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ
                thinking_lines.append(line)
            elif not found_answer_start and not thinking_lines:
                # –ï—Å–ª–∏ –µ—â—ë –Ω–µ –Ω–∞—à–ª–∏ thinking –∏ –Ω–µ –Ω–∞—à–ª–∏ answer - –ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å
                # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ - —ç—Ç–æ answer
                if re.search(
                    r"\d+.*¬∞[–°C]|—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç|—Ä–∞–≤–Ω–∞", line_stripped, re.IGNORECASE
                ):
                    found_answer_start = True
                    answer_lines.append(line)
                else:
                    thinking_lines.append(line)
            else:
                # –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —Å–ª—É—á–∞–π - –¥–æ–±–∞–≤–ª—è–µ–º –≤ thinking, –µ—Å–ª–∏ –æ–Ω —É–∂–µ –Ω–∞—á–∞–ª—Å—è
                if thinking_lines:
                    thinking_lines.append(line)
                else:
                    answer_lines.append(line)

        thinking = "\n".join(thinking_lines).strip()
        answer = "\n".join(answer_lines).strip()

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∏–ª–∏ thinking —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - —Å—á–∏—Ç–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–æ–º
        if not answer or (thinking and len(thinking) > len(answer) * 1.5):
            return "", text

        return thinking, answer

    def _print_answer(
        self,
        title: str,
        text: str,
        chunks: list[RetrievedChunk],
    ) -> None:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º thinking –∏ –æ—Ç–≤–µ—Ç–∞."""
        console.print(Rule(f"[bold blue]{title}[/bold blue]"))

        # –†–∞–∑–¥–µ–ª—è–µ–º thinking –∏ answer
        thinking, answer = self._split_thinking_and_answer(text)

        # –í—ã–≤–æ–¥–∏–º thinking –æ—Ç–¥–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å
        if thinking:
            console.print(
                Panel(
                    thinking,
                    title="[dim italic]–†–∞–∑–º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏[/dim italic]",
                    border_style="dim",
                    style="dim italic",
                    expand=False,
                )
            )
            console.print()

        # –í—ã–≤–æ–¥–∏–º —Å–∞–º –æ—Ç–≤–µ—Ç –±–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω–æ
        if answer:
            console.print(
                Panel(
                    Markdown(answer),
                    title="[bold green]–û—Ç–≤–µ—Ç[/bold green]",
                    border_style="green",
                    expand=True,
                )
            )
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –≤—ã–≤–æ–¥–∏–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ—Ç–≤–µ—Ç
            console.print(
                Panel(
                    Markdown(text),
                    title="[bold green]–û—Ç–≤–µ—Ç[/bold green]",
                    border_style="green",
                    expand=True,
                )
            )

        # –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if chunks:
            console.print(Rule("[bold]–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏[/bold]"))

            seen = {}
            for chunk in chunks:
                key = chunk.file_name
                if key not in seen:
                    seen[key] = {"pages": set(), "sections": set()}
                seen[key]["pages"].add(chunk.page)
                seen[key]["sections"].add(chunk.section)

            table = Table(show_header=True, header_style="bold")
            table.add_column("–ò—Å—Ç–æ—á–Ω–∏–∫", width=50)
            table.add_column("–°—Ç—Ä–∞–Ω–∏—Ü—ã", width=15)
            table.add_column("–°–µ–∫—Ü–∏–∏", width=20)

            for fname, info in sorted(seen.items()):
                pages = ", ".join(map(str, sorted(info["pages"])))
                sections = ", ".join(sorted(info["sections"]))
                table.add_row(fname, pages, sections)

            console.print(table)


# ============ CLI ============

if __name__ == "__main__":
    agent = LiteratureAgent(llm)

    # –ü—Ä–∏–º–µ—Ä: –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
    # agent.answer_question(
    #     "–ö–∞–∫–æ–≤–∞ —Å—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏—è?",
    #     n_results=5,
    # )
    # agent.answer_question(
    #     "–ö–∞–∫ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–≥–æ–¥–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –≤–æ–∑–¥—É—Ö–∞ –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ–¥–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–æ—Ä–Ω—ã—Ö –ø–æ—Ä–æ–¥ –Ω–∞ –≥–ª—É–±–∏–Ω–µ 1 –∏ 4 –º?",
    #     n_results=5,
    # )
    agent.review_topic("–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∑–¥–∞–Ω–∏–π –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π")
